# Credits Brasil - Data Warehouse

DocumentaÃ§Ã£o oficial do Data Warehouse da Credits Brasil para o time de Engenharia de Dados.

## SumÃ¡rio

1. [VisÃ£o Geral do Projeto](#visÃ£o-geral-do-projeto)
2. [Arquitetura Medallion](#arquitetura-medallion)
3. [Modelo Dimensional (Star Schema)](#modelo-dimensional-star-schema)
4. [Camada Bronze - IngestÃ£o de Dados](#camada-bronze---ingestÃ£o-de-dados)
5. [Camada Silver - TransformaÃ§Ãµes](#camada-silver---transformaÃ§Ãµes)
6. [Qualidade de Dados e ValidaÃ§Ãµes](#qualidade-de-dados-e-validaÃ§Ãµes)
7. [Como Executar o Pipeline](#como-executar-o-pipeline)
8. [Estrutura de Arquivos](#estrutura-de-arquivos)
9. [Troubleshooting](#troubleshooting)
10. [GlossÃ¡rio TÃ©cnico](#glossÃ¡rio-tÃ©cnico)

---

## VisÃ£o Geral do Projeto

O Data Warehouse da Credits Brasil Ã© um sistema de consolidaÃ§Ã£o e organizaÃ§Ã£o de dados financeiros para anÃ¡lises estratÃ©gicas. O projeto implementa um pipeline ETL (Extract, Transform, Load) que processa dados provenientes de arquivos CSV e os estrutura em um modelo dimensional otimizado para anÃ¡lises e relatÃ³rios gerenciais.

### Objetivos

- Centralizar dados de clientes, usuÃ¡rios e faturamento em uma Ãºnica fonte de verdade
- Implementar histÃ³rico de mudanÃ§as (SCD Type 2) para rastreamento temporal
- Preparar dados para ferramentas de Business Intelligence
- Garantir qualidade e integridade dos dados atravÃ©s de validaÃ§Ãµes automatizadas

### Tecnologias Utilizadas

- **Linguagem**: Python 3.10+
- **Banco de Dados**: PostgreSQL 15 (Azure Database for PostgreSQL)
- **ConteinerizaÃ§Ã£o**: Docker e Docker Compose
- **Bibliotecas Python**: pandas, psycopg2, loguru, tenacity
- **PadrÃµes de Design**: Template Method, Context Managers, Dataclasses

---

## Arquitetura Medallion

O projeto implementa a arquitetura Medallion com duas camadas: Bronze (dados brutos) e Silver (dados refinados). Esta abordagem incremental permite rastreabilidade, reprocessamento e evoluÃ§Ã£o gradual dos dados.

### Camada Bronze (Raw Data)

A camada Bronze armazena dados brutos dos arquivos CSV com transformaÃ§Ãµes mÃ­nimas.

**CaracterÃ­sticas:**
- EstratÃ©gia de carga: TRUNCATE/RELOAD (substituiÃ§Ã£o completa a cada execuÃ§Ã£o)
- TransformaÃ§Ãµes aplicadas: apenas formataÃ§Ã£o de datas e renomeaÃ§Ã£o de colunas
- Esquema de banco: `bronze`
- Tabelas:
  - `bronze.contas_base_oficial` - Cadastro de clientes
  - `bronze.usuarios` - UsuÃ¡rios da equipe comercial
  - `bronze.faturamento` - TransaÃ§Ãµes de receita
  - `bronze.data` - DimensÃ£o de tempo prÃ©-calculada (4.018 datas)

**Objetivo:** Manter uma cÃ³pia fiel dos dados de origem para auditoria e reprocessamento.

### Camada Silver (Curated Data)

A camada Silver transforma dados brutos em um modelo dimensional (Star Schema) com regras de negÃ³cio aplicadas.

**CaracterÃ­sticas:**
- EstratÃ©gia de carga: FULL (para dimensÃµes simples) e SCD Type 2 (para dimensÃµes com histÃ³rico)
- TransformaÃ§Ãµes aplicadas: limpeza, enriquecimento, cÃ¡lculos, deduplicaÃ§Ã£o
- Esquema de banco: `silver`
- Tabelas:
  - `silver.dim_clientes` - DimensÃ£o de clientes (SCD Type 2)
  - `silver.dim_usuarios` - DimensÃ£o de usuÃ¡rios (SCD Type 2)
  - `silver.dim_tempo` - DimensÃ£o de tempo
  - `silver.dim_canal` - DimensÃ£o de canais de venda
  - `silver.fact_faturamento` - Fato de faturamento

**Objetivo:** Fornecer dados confiÃ¡veis, consistentes e otimizados para anÃ¡lises.

### Fluxo de Dados

```
CSVs (OneDrive)
    |
    v
[Ingestores Bronze]
    |
    v
Bronze Layer (PostgreSQL)
    |
    v
[Transformadores Silver]
    |
    v
Silver Layer (PostgreSQL)
    |
    v
Ferramentas de BI / AnÃ¡lises SQL
```

---

## Modelo Dimensional (Star Schema)

### O que Ã© Star Schema

Star Schema (Esquema Estrela) Ã© um modelo de dados otimizado para consultas analÃ­ticas. Organiza dados em dois tipos de tabelas:

1. **Tabela Fato (Fact Table)**: Centro do modelo, contÃ©m mÃ©tricas numÃ©ricas e chaves estrangeiras
2. **Tabelas DimensÃ£o (Dimension Tables)**: ContÃªm atributos descritivos e contexto

### Vantagens do Star Schema

- Queries SQL mais simples e intuitivas
- Performance superior em agregaÃ§Ãµes
- Facilita a compreensÃ£o por analistas de negÃ³cio
- CompatÃ­vel com ferramentas de BI modernas

### Estrutura do Modelo

```
FACT_FATURAMENTO (Tabela Central)
â”œâ”€â”€ sk_cliente     â†’ DIM_CLIENTES (Quem comprou?)
â”œâ”€â”€ sk_usuario     â†’ DIM_USUARIOS (Quem vendeu?)
â”œâ”€â”€ sk_data        â†’ DIM_TEMPO (Quando vendeu?)
â””â”€â”€ sk_canal       â†’ DIM_CANAL (Por qual canal?)
```

### DescriÃ§Ã£o das Tabelas

#### FACT_FATURAMENTO

Tabela de fatos que armazena transaÃ§Ãµes de faturamento com mÃ©tricas calculadas.

**Campos principais:**
- `sk_faturamento` - Chave substituta (PK, autoincrement)
- `sk_cliente` - FK para dim_clientes
- `sk_usuario` - FK para dim_usuarios
- `sk_data` - FK para dim_tempo
- `sk_canal` - FK para dim_canal
- `valor_bruto` - Receita antes de descontos
- `valor_desconto` - Valor de descontos aplicados
- `valor_liquido` - Receita final (bruto - desconto)
- `valor_imposto` - Impostos calculados (15% do bruto)
- `valor_comissao` - ComissÃ£o do vendedor (5% do bruto)
- `moeda` - CÃ³digo da moeda (BRL, USD, EUR)
- `forma_pagamento` - Forma de pagamento
- `status_pagamento` - Status atual do pagamento
- `hash_transacao` - Hash MD5 para detectar duplicatas

#### DIM_CLIENTES

DimensÃ£o de clientes com suporte a histÃ³rico de mudanÃ§as (SCD Type 2).

**Campos principais:**
- `sk_cliente` - Chave substituta (PK, autoincrement)
- `nk_cnpj_cpf` - Chave natural (CNPJ ou CPF limpo)
- `razao_social` - Nome/RazÃ£o Social
- `tipo_pessoa` - PF (Pessoa FÃ­sica) ou PJ (Pessoa JurÃ­dica)
- `status` - Status atual da conta
- `grupo` - Grupo empresarial
- `responsavel_conta` - Account Manager
- `tempo_cliente_dias` - Tempo como cliente em dias
- `categoria_risco` - ClassificaÃ§Ã£o de risco
- Campos SCD Type 2:
  - `data_inicio` - Data de vigÃªncia inicial
  - `data_fim` - Data de vigÃªncia final (NULL para registro ativo)
  - `flag_ativo` - TRUE para versÃ£o atual
  - `versao` - NÃºmero sequencial da versÃ£o
  - `hash_registro` - Hash MD5 dos dados para detectar mudanÃ§as
  - `motivo_mudanca` - DescriÃ§Ã£o da mudanÃ§a realizada

#### DIM_USUARIOS

DimensÃ£o de usuÃ¡rios comerciais com hierarquia de gestores e histÃ³rico.

**Campos principais:**
- `sk_usuario` - Chave substituta (PK, autoincrement)
- `nk_usuario` - Chave natural (email limpo)
- `nome_completo` - Nome do usuÃ¡rio
- `email` - Email corporativo
- `area` - Ãrea de atuaÃ§Ã£o (Vendas, Comercial, TI)
- `senioridade` - NÃ­vel hierÃ¡rquico (Junior, Pleno, Senior)
- `sk_gestor` - FK para dim_usuarios (auto-relacionamento)
- `nome_gestor` - Nome do gestor direto
- `canal_1`, `canal_2` - Canais de vendas associados
- Campos SCD Type 2: mesma estrutura de dim_clientes

#### DIM_TEMPO

DimensÃ£o de tempo prÃ©-calculada para anÃ¡lises temporais eficientes.

**Campos principais:**
- `sk_data` - Chave substituta (PK, autoincrement)
- `data_completa` - Data completa (UNIQUE)
- `ano`, `mes`, `dia` - Componentes da data
- `trimestre` - NÃºmero do trimestre (1-4)
- `semestre` - NÃºmero do semestre (1-2)
- `nome_mes` - Nome do mÃªs em portuguÃªs
- `nome_dia_semana` - Nome do dia da semana
- `numero_semana` - NÃºmero da semana no ano
- `dia_util` - Flag indicando se Ã© dia Ãºtil

#### DIM_CANAL

DimensÃ£o de canais de venda.

**Campos principais:**
- `sk_canal` - Chave substituta (PK, autoincrement)
- `tipo_canal` - Tipo (Direto, Indireto)
- `nome_canal` - Nome especÃ­fico (Inside Sales, Field Sales, Parceiro, etc.)

---

## Camada Bronze - IngestÃ£o de Dados

### Como Funciona a IngestÃ£o

Os ingestores Bronze sÃ£o scripts Python que leem arquivos CSV e carregam os dados no banco de dados com transformaÃ§Ãµes mÃ­nimas. Todos os ingestores seguem o padrÃ£o Template Method atravÃ©s da classe base `BaseCSVIngestor`.

### PadrÃ£o Template Method

A classe `BaseCSVIngestor` define o fluxo de execuÃ§Ã£o padrÃ£o:

1. Validar se o arquivo existe e tem tamanho adequado
2. Conectar ao banco de dados
3. Registrar execuÃ§Ã£o na tabela de auditoria
4. Ler o arquivo CSV
5. Validar se todas as colunas obrigatÃ³rias estÃ£o presentes
6. Transformar dados (aplicar mapeamento de colunas e formataÃ§Ã£o de datas)
7. Inserir dados no banco (TRUNCATE + INSERT)
8. Mover arquivo para pasta de processados
9. Finalizar registro de auditoria

### Criando um Novo Ingestor (VersÃ£o 2.0)

Para criar um ingestor para um novo arquivo CSV, vocÃª deve implementar 3 mÃ©todos obrigatÃ³rios:

```python
from ingestors.csv.base_csv_ingestor import BaseCSVIngestor
from typing import Dict, List

class IngestMeuArquivo(BaseCSVIngestor):
    """Ingestor para meu arquivo CSV"""

    def __init__(self):
        super().__init__(
            script_name='ingest_meu_arquivo.py',
            tabela_destino='bronze.minha_tabela',
            arquivo_nome='meu_arquivo.csv',
            input_subdir='onedrive'
        )

    def get_column_mapping(self) -> Dict[str, str]:
        """Mapeia colunas do CSV para colunas do banco"""
        return {
            'Coluna CSV 1': 'coluna_banco_1',
            'Coluna CSV 2': 'coluna_banco_2',
            'Email': 'email'
        }

    def get_bronze_columns(self) -> List[str]:
        """Lista colunas da tabela Bronze (excluindo sk_id autoincrement)"""
        return ['coluna_banco_1', 'coluna_banco_2', 'email']

    def get_validation_rules(self) -> Dict[str, dict]:
        """
        Regras de validaÃ§Ã£o para cada campo Bronze.
        OBRIGATÃ“RIO na versÃ£o 2.0.
        """
        return {
            # Campo obrigatÃ³rio com tamanho mÃ­nimo
            'coluna_banco_1': {
                'obrigatorio': True,
                'tipo': 'string',
                'min_len': 3,
                'max_len': 100
            },

            # Campo opcional numÃ©rico positivo
            'coluna_banco_2': {
                'obrigatorio': False,
                'tipo': 'decimal',
                'positivo': True
            },

            # Email obrigatÃ³rio
            'email': {
                'obrigatorio': True,
                'tipo': 'email'
            }
        }

    def get_date_columns(self) -> List[str]:
        """Opcional: lista colunas de data para formataÃ§Ã£o automÃ¡tica"""
        return []  # Adicione nomes de colunas de data se houver
```

**Tipos de validaÃ§Ã£o disponÃ­veis:**
- `obrigatorio`: True/False - campo deve ter valor
- `tipo`: 'string', 'int', 'float', 'decimal', 'data', 'email', 'cnpj_cpf'
- `min_len` / `max_len`: tamanho da string
- `minimo` / `maximo`: range numÃ©rico
- `positivo`: True - nÃºmero deve ser > 0
- `nao_negativo`: True - nÃºmero deve ser >= 0
- `dominio`: lista de valores permitidos (ex: ['BRL', 'USD', 'EUR'])
- `case_sensitive`: True/False - para validaÃ§Ã£o de domÃ­nio
- `formato_data`: '%Y-%m-%d' - formato de data esperado

### TransformaÃ§Ãµes e ValidaÃ§Ãµes Aplicadas na Bronze (VersÃ£o 2.0)

#### Fluxo de Processamento

1. **Leitura do CSV**: Arquivo Ã© lido com pandas (todas colunas como string inicialmente)
2. **ValidaÃ§Ã£o de Estrutura**: Verifica se todas colunas esperadas estÃ£o presentes
3. **RenomeaÃ§Ã£o de Colunas**: Mapeamento CSV â†’ Bronze aplicado
4. **ValidaÃ§Ã£o Linha por Linha**: Cada registro validado contra regras definidas
5. **RejeiÃ§Ã£o de InvÃ¡lidos**: Registros problemÃ¡ticos sÃ£o rejeitados e logados
6. **FormataÃ§Ã£o de Datas**: Apenas datas vÃ¡lidas sÃ£o formatadas para YYYY-MM-DD
7. **TransformaÃ§Ãµes Customizadas**: Aplicadas via `transform_custom()` se necessÃ¡rio
8. **InserÃ§Ã£o em Lote**: Apenas dados vÃ¡lidos inseridos (TRUNCATE/RELOAD)
9. **Log de RejeiÃ§Ãµes**: Registros rejeitados salvos em `credits.logs_rejeicao`
10. **Arquivamento**: Arquivo movido para `data/processed/` com timestamp

#### ValidaÃ§Ãµes Rigorosas Aplicadas

**Campos ObrigatÃ³rios:**
- Valores vazios, nulos ou apenas espaÃ§os sÃ£o REJEITADOS
- Mensagem: "Campo obrigatÃ³rio '{campo}' estÃ¡ vazio ou nulo"

**ValidaÃ§Ã£o de Formato:**
- **Datas**: Devem estar em formato vÃ¡lido e conversÃ­vel
- **Emails**: ValidaÃ§Ã£o de formato com regex
- **CNPJ/CPF**: ValidaÃ§Ã£o de dÃ­gitos verificadores
- **NÃºmeros**: Devem ser conversÃ­veis para int/float/decimal

**ValidaÃ§Ã£o de DomÃ­nio:**
- Valores devem estar em lista prÃ©-definida (ex: moedas, status)
- Case-sensitive ou nÃ£o, conforme configuraÃ§Ã£o

**ValidaÃ§Ã£o de Ranges:**
- NÃºmeros podem ter valores mÃ­nimos/mÃ¡ximos
- Strings podem ter tamanhos mÃ­nimos/mÃ¡ximos

**Importante:** Na versÃ£o 2.0, a Bronze **NÃƒO** preserva dados invÃ¡lidos.
Apenas registros 100% vÃ¡lidos sÃ£o inseridos no banco de dados.

### Auditoria de ExecuÃ§Ãµes

Todas as execuÃ§Ãµes sÃ£o rastreadas na tabela `credits.historico_atualizacoes`:

```sql
SELECT
    id_execucao,
    script_nome,
    status,
    linhas_processadas,
    linhas_inseridas,
    data_inicio,
    data_fim,
    tempo_execucao
FROM credits.historico_atualizacoes
ORDER BY data_inicio DESC
LIMIT 10;
```

### LocalizaÃ§Ã£o dos Arquivos

**Dentro do container Docker:**
- Arquivos de entrada: `/app/data/input/onedrive/`
- Arquivos processados: `/app/data/processed/`
- Logs: `/app/logs/`

**No sistema host:**
- Arquivos de entrada: `docker/data/input/onedrive/`
- Arquivos processados: `docker/data/processed/`
- Logs: `logs/` (na raiz do projeto)

---

## Camada Silver - TransformaÃ§Ãµes

### Como Funcionam as TransformaÃ§Ãµes

Os transformadores Silver leem dados da camada Bronze, aplicam regras de negÃ³cio, calculam campos derivados, validam qualidade e carregam na camada Silver. Todos os transformadores seguem o padrÃ£o Template Method atravÃ©s da classe base `BaseSilverTransformer`.

### PadrÃ£o Template Method Silver

A classe `BaseSilverTransformer` define o fluxo de execuÃ§Ã£o:

1. Extrair dados da camada Bronze
2. Aplicar transformaÃ§Ãµes e regras de negÃ³cio
3. Validar qualidade dos dados
4. Processar conforme estratÃ©gia de carga (FULL ou SCD Type 2)
5. Inserir dados na camada Silver
6. Logar mÃ©tricas de execuÃ§Ã£o

### EstratÃ©gias de Carga

#### FULL Load

Utilizada para tabelas que nÃ£o requerem histÃ³rico. A cada execuÃ§Ã£o:
- TRUNCATE na tabela de destino
- INSERT de todos os registros transformados

**Exemplo:** `dim_canal`, `fact_faturamento`

#### SCD Type 2 (Slowly Changing Dimension Type 2)

Utilizada para dimensÃµes que requerem histÃ³rico de mudanÃ§as.

**Como funciona:**

1. **Primeira Carga**: Todos os registros sÃ£o inseridos com:
   - `data_inicio` = data atual
   - `data_fim` = NULL
   - `flag_ativo` = TRUE
   - `versao` = 1

2. **Cargas Subsequentes**:
   - Para cada registro, calcula hash MD5 dos dados
   - Compara com hash do registro ativo no banco
   - Se hash diferente:
     - Fecha registro antigo (atualiza `data_fim`, `flag_ativo` = FALSE)
     - Insere nova versÃ£o (incrementa `versao`, `flag_ativo` = TRUE)
   - Se hash igual: nenhuma aÃ§Ã£o (dados nÃ£o mudaram)
   - Se registro novo (chave natural nÃ£o existe): insere com versÃ£o 1

**Exemplo prÃ¡tico:**

```
Cliente CNPJ 12.345.678/0001-99 muda status de ATIVO para INATIVO

Antes:
sk_cliente | nk_cnpj_cpf        | status | data_inicio | data_fim | flag_ativo | versao
1          | 12345678000199     | ATIVO  | 2024-01-01  | NULL     | TRUE       | 1

Depois:
sk_cliente | nk_cnpj_cpf        | status   | data_inicio | data_fim   | flag_ativo | versao
1          | 12345678000199     | ATIVO    | 2024-01-01  | 2024-06-30 | FALSE      | 1
2          | 12345678000199     | INATIVO  | 2024-07-01  | NULL       | TRUE       | 2
```

**Exemplo:** `dim_clientes`, `dim_usuarios`

### TransformaÃ§Ãµes EspecÃ­ficas

#### TransformDimClientes

**Origem:** `bronze.contas_base_oficial`
**Destino:** `silver.dim_clientes`
**Tipo:** SCD Type 2

**TransformaÃ§Ãµes aplicadas:**
1. Limpeza de CNPJ/CPF (remove caracteres nÃ£o numÃ©ricos)
2. DeterminaÃ§Ã£o de tipo de pessoa (PF se <= 11 dÃ­gitos, PJ se > 11)
3. CÃ¡lculo de tempo como cliente em dias
4. ClassificaÃ§Ã£o de porte de empresa (fixo: MEDIO)
5. ClassificaÃ§Ã£o de risco (fixo: BAIXO)
6. FormataÃ§Ã£o de data de criaÃ§Ã£o
7. CÃ¡lculo de hash para detecÃ§Ã£o de mudanÃ§as

**ValidaÃ§Ãµes:**
- CNPJ/CPF nÃ£o pode ser nulo
- NÃ£o pode haver CNPJ/CPF duplicados no mesmo batch

#### TransformDimUsuarios

**Origem:** `bronze.usuarios`
**Destino:** `silver.dim_usuarios`
**Tipo:** SCD Type 2

**TransformaÃ§Ãµes aplicadas:**
1. CriaÃ§Ã£o de chave natural a partir do email
2. PadronizaÃ§Ã£o de senioridade
3. ResoluÃ§Ã£o de hierarquia de gestores (auto-relacionamento)
4. NormalizaÃ§Ã£o de canais de vendas
5. CÃ¡lculo de hash para detecÃ§Ã£o de mudanÃ§as

**ValidaÃ§Ãµes:**
- Email nÃ£o pode ser nulo
- NÃ£o pode haver emails duplicados no mesmo batch

#### TransformFactFaturamento

**Origem:** `bronze.faturamento`
**Destino:** `silver.fact_faturamento`
**Tipo:** FULL Load

**TransformaÃ§Ãµes aplicadas:**
1. ConversÃ£o de data para date
2. Lookup de chaves estrangeiras:
   - `sk_cliente` via JOIN com `dim_clientes` (flag_ativo = TRUE)
   - `sk_usuario` via JOIN com `dim_usuarios` (flag_ativo = TRUE)
   - `sk_data` via JOIN com `dim_tempo`
   - `sk_canal` via JOIN com `dim_canal`
3. CÃ¡lculos de mÃ©tricas:
   - `valor_bruto` = receita original
   - `valor_liquido` = valor_bruto - valor_desconto
   - `valor_imposto` = valor_bruto * 0.15
   - `valor_comissao` = valor_bruto * 0.05
4. PadronizaÃ§Ã£o de campos (tipo_documento, forma_pagamento)
5. CÃ¡lculo de hash da transaÃ§Ã£o

**ValidaÃ§Ãµes:**
- Todas as chaves estrangeiras devem ser resolvidas (nÃ£o podem ser NULL)
- Valores monetÃ¡rios nÃ£o podem ser nulos

---

## Qualidade de Dados e ValidaÃ§Ãµes

### âš¡ MudanÃ§a Arquitetural Importante (VersÃ£o 2.0)

**A partir da versÃ£o 2.0, a camada Bronze implementa validaÃ§Ã£o RIGOROSA.**
**Apenas dados VÃLIDOS sÃ£o inseridos no banco de dados.**
**Dados invÃ¡lidos sÃ£o REJEITADOS e registrados para auditoria.**

### NÃ­veis de ValidaÃ§Ã£o

#### Bronze Layer (RIGOROSA - Nova Arquitetura)

A camada Bronze agora REJEITA dados invÃ¡lidos ANTES da inserÃ§Ã£o:

- âœ… **Campos obrigatÃ³rios**: Devem estar preenchidos (nÃ£o aceita NULL/vazio)
- âœ… **Datas**: Devem ser vÃ¡lidas no formato YYYY-MM-DD
- âœ… **NÃºmeros**: Devem ser vÃ¡lidos e nÃ£o-negativos quando apropriado
- âœ… **Emails**: Devem ter formato vÃ¡lido (regex)
- âœ… **CNPJ/CPF**: Devem ser vÃ¡lidos (dÃ­gitos verificadores)
- âœ… **DomÃ­nios**: Devem estar na lista de valores permitidos

**Registros rejeitados sÃ£o registrados em `credits.logs_rejeicao`** para anÃ¡lise e correÃ§Ã£o.

**Objetivo:** Garantir que apenas dados de qualidade entrem no Data Warehouse desde a origem.

#### Silver Layer (Refinamento e TransformaÃ§Ãµes)

A camada Silver aplica regras de negÃ³cio e transformaÃ§Ãµes:

- CNPJ/CPF nulo: REJEITA execuÃ§Ã£o
- CNPJ/CPF duplicado: REJEITA execuÃ§Ã£o
- Chave estrangeira nÃ£o encontrada: REJEITA execuÃ§Ã£o
- Valores monetÃ¡rios nulos: REJEITA execuÃ§Ã£o

**Objetivo:** Garantir integridade e confiabilidade dos dados analÃ­ticos.

### Sistema de Logs de RejeiÃ§Ã£o (VersÃ£o 2.0)

#### Tabela de Logs: `credits.logs_rejeicao`

Todos os registros rejeitados pela camada Bronze sÃ£o registrados em uma tabela dedicada para auditoria e correÃ§Ã£o.

**Estrutura da Tabela:**
```sql
credits.logs_rejeicao
â”œâ”€â”€ id (BIGSERIAL PK)
â”œâ”€â”€ execucao_id (UUID) -- FK para credits.historico_atualizacoes
â”œâ”€â”€ script_nome (VARCHAR) -- Nome do ingestor que rejeitou
â”œâ”€â”€ tabela_destino (VARCHAR) -- Tabela Bronze de destino
â”œâ”€â”€ numero_linha (INTEGER) -- Linha no arquivo CSV original
â”œâ”€â”€ campo_falha (VARCHAR) -- Campo que falhou na validaÃ§Ã£o
â”œâ”€â”€ motivo_rejeicao (TEXT) -- DescriÃ§Ã£o clara do motivo
â”œâ”€â”€ valor_recebido (TEXT) -- Valor que causou a falha
â”œâ”€â”€ registro_completo (JSONB) -- Registro completo para anÃ¡lise
â”œâ”€â”€ severidade (VARCHAR) -- WARNING, ERROR ou CRITICAL
â””â”€â”€ data_rejeicao (TIMESTAMP) -- Quando ocorreu a rejeiÃ§Ã£o
```

#### Consultando RejeiÃ§Ãµes

**Ver Ãºltimas rejeiÃ§Ãµes de uma execuÃ§Ã£o:**
```sql
SELECT
    numero_linha,
    campo_falha,
    motivo_rejeicao,
    valor_recebido
FROM credits.logs_rejeicao
WHERE execucao_id = 'UUID_DA_EXECUCAO'
ORDER BY numero_linha;
```

**Resumo de rejeiÃ§Ãµes por campo:**
```sql
SELECT
    campo_falha,
    motivo_rejeicao,
    COUNT(*) as total_rejeicoes
FROM credits.logs_rejeicao
WHERE script_nome = 'ingest_faturamento.py'
    AND data_rejeicao >= NOW() - INTERVAL '7 days'
GROUP BY campo_falha, motivo_rejeicao
ORDER BY total_rejeicoes DESC;
```

**Ver registro completo de rejeiÃ§Ã£o:**
```sql
SELECT
    registro_completo::jsonb
FROM credits.logs_rejeicao
WHERE id = 123;
```

#### Como Funciona a RejeiÃ§Ã£o

1. **ValidaÃ§Ã£o Linha por Linha**: Cada registro do CSV Ã© validado campo por campo
2. **Primeira Falha Rejeita**: Ao encontrar um campo invÃ¡lido, o registro Ã© rejeitado imediatamente
3. **Log Estruturado**: Detalhes da rejeiÃ§Ã£o sÃ£o registrados na tabela
4. **Resumo no Console**: Ao final, um resumo Ã© exibido:
   ```
   âš ï¸  RESUMO DE REJEIÃ‡Ã•ES: 15 registros rejeitados
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ“Š RejeiÃ§Ãµes por campo:
      â€¢ cnpj_cliente: 8 rejeiÃ§Ãµes
      â€¢ email_usuario: 5 rejeiÃ§Ãµes
      â€¢ receita: 2 rejeiÃ§Ãµes
   ğŸ” RejeiÃ§Ãµes por severidade:
      â€¢ ERROR: 15 rejeiÃ§Ãµes
   ```
5. **Apenas Dados VÃ¡lidos Inseridos**: Somente registros que passaram em TODAS as validaÃ§Ãµes sÃ£o inseridos

#### Exemplo de RejeiÃ§Ã£o

**CSV de Entrada:**
```csv
Data,Receita,Moeda,CNPJ Cliente,Email Usuario
2024-01-15,15000.50,BRL,12.345.678/0001-90,joao.silva@empresa.com
2024-01-20,-5000.00,BRL,98.765.432/0001-10,maria@invalid  <- INVÃLIDO
2024-02-10,18500.75,XXX,INVALIDO,pedro.costa@empresa.com  <- INVÃLIDOS
```

**Resultado:**
- âœ… Linha 2: Inserida (todos campos vÃ¡lidos)
- âŒ Linha 3: Rejeitada (receita negativa, email invÃ¡lido)
- âŒ Linha 4: Rejeitada (moeda invÃ¡lida, CNPJ invÃ¡lido)

**Log gerado:**
```
âŒ REJEIÃ‡ÃƒO | Linha 3 | Campo 'receita' | NÃºmero deve ser positivo (> 0), recebido: -5000.00
âŒ REJEIÃ‡ÃƒO | Linha 4 | Campo 'moeda' | Valor 'XXX' nÃ£o estÃ¡ no domÃ­nio permitido: ['BRL', 'USD', 'EUR']
```

### Logs de Qualidade

Todos os ingestores e transformadores registram detalhadamente:

```
INFO: Shape: 10 linhas x 3 colunas
WARNING: Valores nulos detectados:
  - Data: 2 (20.0%)
  - Receita: 2 (20.0%)
  - Moeda: 2 (20.0%)
WARNING: 'data': 3 datas invÃ¡lidas convertidas para NULL
```

LocalizaÃ§Ã£o dos logs: `/app/logs/` (dentro do container) ou `logs/` (no host).

---

## Como Executar o Pipeline

### PrÃ©-requisitos

1. Docker e Docker Compose instalados
2. Acesso Ã  internet para conectar ao PostgreSQL Azure
3. Credenciais do banco de dados configuradas
4. Arquivos CSV disponÃ­veis

### ConfiguraÃ§Ã£o Inicial

#### 1. Clonar o RepositÃ³rio

```bash
git clone https://github.com/brunocredits/credits-dw.git
cd credits-dw
```

#### 2. Configurar VariÃ¡veis de Ambiente

Copie o arquivo de exemplo e edite com as credenciais reais:

```bash
cp .env.example .env
```

Edite `.env`:
```
DB_HOST=seu_host.postgres.database.azure.com
DB_PORT=5432
DB_NAME=creditsdw
DB_USER=seu_usuario
DB_PASSWORD=sua_senha
```

**IMPORTANTE:** O arquivo `.env` estÃ¡ no `.gitignore` e nunca deve ser commitado.

#### 3. Preparar Arquivos CSV

Coloque os arquivos CSV na pasta de entrada:

```bash
# Estrutura esperada:
docker/data/input/onedrive/
â”œâ”€â”€ contas_base_oficial.csv
â”œâ”€â”€ usuarios.csv
â””â”€â”€ faturamento.csv
```

**Formato dos CSVs:**

- **Separador:** ponto e vÃ­rgula (;)
- **Encoding:** UTF-8
- **CabeÃ§alho:** Primeira linha deve conter nomes das colunas

### ExecuÃ§Ã£o Passo a Passo

#### 1. Iniciar o Container

```bash
cd docker
docker compose up -d --build
```

Verificar se o container estÃ¡ rodando:
```bash
docker compose ps
```

Deve mostrar `etl-processor` com status `running`.

#### 2. Executar IngestÃ£o Bronze

**OpÃ§Ã£o A: Executar todos os ingestores**

```bash
docker compose exec etl-processor python python/run_all_ingestors.py
```

**OpÃ§Ã£o B: Executar ingestores individuais**

```bash
# Ingerir contas
docker compose exec etl-processor python python/ingestors/csv/ingest_contas_base_oficial.py

# Ingerir usuÃ¡rios
docker compose exec etl-processor python python/ingestors/csv/ingest_usuarios.py

# Ingerir faturamento
docker compose exec etl-processor python python/ingestors/csv/ingest_faturamento.py
```

**SaÃ­da esperada:**
```
ğŸš€ Iniciando ingestÃ£o: ingest_contas_base_oficial.py
âœ“ Arquivo vÃ¡lido | Tamanho: 0.00 MB
âœ“ ConexÃ£o com banco estabelecida
âœ“ Arquivo lido com sucesso | 10 linhas
âœ“ Todas as colunas obrigatÃ³rias estÃ£o presentes
âœ“ TransformaÃ§Ã£o concluÃ­da | 10 registros preparados
âœ“ InserÃ§Ã£o concluÃ­da | 10 linhas inseridas
âœ… EXECUÃ‡ÃƒO CONCLUÃDA COM SUCESSO
```

#### 3. Executar TransformaÃ§Ãµes Silver

```bash
docker compose exec etl-processor python python/run_silver_transformations.py
```

**SaÃ­da esperada:**
```
=== Executando TransformaÃ§Ãµes Silver ===

â–¶ Executando dim_clientes...
âœ“ dim_clientes concluÃ­do

â–¶ Executando dim_usuarios...
âœ“ dim_usuarios concluÃ­do

â–¶ Executando fact_faturamento...
âœ“ fact_faturamento concluÃ­do

=== Todas transformaÃ§Ãµes concluÃ­das com sucesso ===
```

#### 4. Validar Resultados

Execute queries SQL para verificar os dados:

```sql
-- Contar registros por tabela
SELECT 'bronze.contas_base_oficial' AS tabela, COUNT(*) AS registros FROM bronze.contas_base_oficial
UNION ALL
SELECT 'silver.dim_clientes', COUNT(*) FROM silver.dim_clientes
UNION ALL
SELECT 'silver.fact_faturamento', COUNT(*) FROM silver.fact_faturamento;

-- Verificar integridade referencial
SELECT
    'Fact â†’ Dim Clientes' AS relacionamento,
    COUNT(*) AS total,
    COUNT(DISTINCT f.sk_cliente) AS chaves_distintas,
    SUM(CASE WHEN c.sk_cliente IS NULL THEN 1 ELSE 0 END) AS fks_orfas
FROM silver.fact_faturamento f
LEFT JOIN silver.dim_clientes c ON f.sk_cliente = c.sk_cliente;
```

### ExecuÃ§Ã£o Agendada (Opcional)

Para executar automaticamente em horÃ¡rios especÃ­ficos, configure um cron job:

```bash
# Editar crontab
crontab -e

# Executar pipeline completo todo dia Ã s 3h da manhÃ£
0 3 * * * cd /home/usuario/credits-dw/docker && docker compose exec -T etl-processor python python/run_all_ingestors.py && docker compose exec -T etl-processor python python/run_silver_transformations.py
```

---

## Estrutura de Arquivos

```
credits-dw/
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile                    # Imagem Python 3.10 com dependÃªncias
â”‚   â”œâ”€â”€ docker-compose.yml            # OrquestraÃ§Ã£o do container
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ input/onedrive/           # CSVs para processar
â”‚       â”œâ”€â”€ processed/                # CSVs jÃ¡ processados (arquivados)
â”‚       â””â”€â”€ templates/                # Exemplos de CSVs
â”‚
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ ingestors/
â”‚   â”‚   â””â”€â”€ csv/
â”‚   â”‚       â”œâ”€â”€ base_csv_ingestor.py        # Classe base (Template Method)
â”‚   â”‚       â”œâ”€â”€ ingest_contas_base_oficial.py
â”‚   â”‚       â”œâ”€â”€ ingest_usuarios.py
â”‚   â”‚       â””â”€â”€ ingest_faturamento.py
â”‚   â”‚
â”‚   â”œâ”€â”€ transformers/
â”‚   â”‚   â”œâ”€â”€ base_transformer.py             # Classe base Silver
â”‚   â”‚   â””â”€â”€ silver/
â”‚   â”‚       â”œâ”€â”€ transform_dim_clientes.py   # SCD Type 2
â”‚   â”‚       â”œâ”€â”€ transform_dim_usuarios.py   # SCD Type 2
â”‚   â”‚       â””â”€â”€ transform_fact_faturamento.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ config.py                 # ConfiguraÃ§Ãµes centralizadas (dataclasses)
â”‚   â”‚   â”œâ”€â”€ db_connection.py          # Gerenciamento de conexÃµes (context managers)
â”‚   â”‚   â”œâ”€â”€ logger.py                 # ConfiguraÃ§Ã£o Loguru
â”‚   â”‚   â””â”€â”€ audit.py                  # FunÃ§Ãµes de auditoria
â”‚   â”‚
â”‚   â”œâ”€â”€ run_all_ingestors.py          # Script para executar todos Bronze
â”‚   â””â”€â”€ run_silver_transformations.py # Script para executar todos Silver
â”‚
â”œâ”€â”€ logs/                              # Logs de execuÃ§Ã£o (criado automaticamente)
â”‚
â”œâ”€â”€ .env                               # Credenciais (NÃƒO versionar, estÃ¡ no .gitignore)
â”œâ”€â”€ .env.example                       # Template de .env
â”œâ”€â”€ requirements.txt                   # DependÃªncias Python
â”œâ”€â”€ README.md                          # Este arquivo
â””â”€â”€ CLAUDE.md                          # DocumentaÃ§Ã£o tÃ©cnica para Claude Code
```

### Arquivos de ConfiguraÃ§Ã£o

#### requirements.txt

Lista de pacotes Python instalados no container:

```
pandas==2.1.4          # ManipulaÃ§Ã£o de DataFrames
psycopg2-binary==2.9.9 # Driver PostgreSQL
loguru==0.7.2          # Sistema de logs avanÃ§ado
tenacity==8.2.3        # Retry logic com exponential backoff
python-dotenv==1.0.0   # Carregamento de variÃ¡veis de ambiente
```

#### docker-compose.yml

ConfiguraÃ§Ã£o do serviÃ§o ETL:

```yaml
services:
  etl-processor:
    build: .
    container_name: credits-etl
    volumes:
      - ../python:/app/python
      - ../logs:/app/logs
      - ./data:/app/data
    environment:
      - DB_HOST=${DB_HOST}
      - DB_PORT=${DB_PORT}
      - DB_NAME=${DB_NAME}
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
      - TZ=America/Sao_Paulo
```

---

## Troubleshooting

### Container nÃ£o inicia

**Sintoma:** `docker compose up -d` falha ou container para imediatamente.

**DiagnÃ³stico:**
```bash
docker compose logs etl-processor
```

**SoluÃ§Ãµes:**
1. Verificar se o arquivo `.env` existe e estÃ¡ configurado
2. Rebuild forÃ§ado: `docker compose down && docker compose up -d --build`
3. Verificar portas em uso: `docker compose ps`

### Erro de conexÃ£o ao banco de dados

**Sintoma:** `connection refused` ou `timeout`

**Causas comuns:**
1. Firewall do Azure bloqueando seu IP
2. Credenciais incorretas no `.env`
3. Banco de dados fora do ar

**DiagnÃ³stico:**
```bash
# Testar conexÃ£o de dentro do container
docker compose exec etl-processor python -c "
from utils.db_connection import get_connection
with get_connection() as conn:
    print('ConexÃ£o OK')
"
```

**SoluÃ§Ãµes:**
1. Adicionar seu IP no firewall do Azure PostgreSQL
2. Verificar credenciais: usuÃ¡rio, senha, nome do banco
3. Testar com `psql` ou ferramenta GUI (DBeaver, pgAdmin)

### Ingestor falha com "Colunas obrigatÃ³rias faltando"

**Sintoma:**
```
ValueError: Colunas obrigatÃ³rias faltando: {'Data de criaÃ§Ã£o', 'CNPJ/CPF PK'}
```

**Causa:** Nome das colunas no CSV nÃ£o corresponde ao mapeamento no ingestor.

**SoluÃ§Ã£o:**
1. Abrir o CSV e verificar os nomes exatos das colunas
2. Comparar com `get_column_mapping()` no arquivo do ingestor
3. Ajustar o CSV ou o mapeamento conforme necessÃ¡rio

**Exemplo:**
```python
# Ingestor espera:
'CNPJ/CPF PK': 'cnpj_cpf'

# Mas CSV tem:
'CNPJ / CPF'  # <-- EspaÃ§os extras

# SoluÃ§Ã£o: Ajustar header do CSV ou mapeamento
```

### TransformaÃ§Ã£o Silver falha com "ValidaÃ§Ã£o falhou"

**Sintomas possÃ­veis:**
- "CNPJs/CPFs duplicados encontrados"
- "sk_cliente nulo - dim_clientes vazia?"
- "Datas nÃ£o encontradas na dim_tempo"

**DiagnÃ³stico:**
```sql
-- Verificar se Bronze tem dados
SELECT COUNT(*) FROM bronze.contas_base_oficial;

-- Verificar duplicatas
SELECT cnpj_cpf, COUNT(*)
FROM bronze.contas_base_oficial
GROUP BY cnpj_cpf
HAVING COUNT(*) > 1;

-- Verificar dim_tempo
SELECT MIN(data_completa), MAX(data_completa), COUNT(*)
FROM silver.dim_tempo;
```

**SoluÃ§Ãµes:**
1. **Duplicatas:** Limpar dados de origem, remover duplicatas antes de reprocessar
2. **DimensÃµes vazias:** Executar ingestÃ£o Bronze primeiro
3. **Datas fora do range:** Estender dim_tempo ou ajustar datas nos CSVs

### Logs nÃ£o aparecem

**Sintoma:** Pasta `logs/` vazia ou arquivos nÃ£o criados.

**Causa:** PermissÃµes de escrita ou volume nÃ£o montado.

**SoluÃ§Ã£o:**
```bash
# Verificar se pasta existe
ls -la logs/

# Criar manualmente se necessÃ¡rio
mkdir -p logs
chmod 777 logs

# Verificar dentro do container
docker compose exec etl-processor ls -la /app/logs
```

### Arquivo CSV nÃ£o Ã© encontrado

**Sintoma:** `Arquivo nÃ£o encontrado: /app/data/input/onedrive/usuarios.csv`

**Causa:** Arquivo nÃ£o estÃ¡ no local esperado ou nome incorreto.

**SoluÃ§Ã£o:**
```bash
# Verificar conteÃºdo da pasta
docker compose exec etl-processor ls -la /app/data/input/onedrive/

# Copiar arquivo para local correto
cp meu_arquivo.csv docker/data/input/onedrive/usuarios.csv

# Verificar nome do arquivo no ingestor
# Deve corresponder exatamente ao parÃ¢metro arquivo_nome no __init__
```

### Performance lenta

**Sintoma:** IngestÃ£o ou transformaÃ§Ã£o demora muito tempo.

**Causas comuns:**
1. Arquivos CSV muito grandes
2. Rede lenta para Azure
3. Queries sem Ã­ndices

**SoluÃ§Ãµes:**
1. Processar em batches menores
2. Usar conexÃ£o de rede mais rÃ¡pida
3. Criar Ã­ndices no banco:
```sql
CREATE INDEX idx_clientes_nk ON silver.dim_clientes(nk_cnpj_cpf);
CREATE INDEX idx_usuarios_nk ON silver.dim_usuarios(nk_usuario);
CREATE INDEX idx_tempo_data ON silver.dim_tempo(data_completa);
```

---

## GlossÃ¡rio TÃ©cnico

### Conceitos de Arquitetura

**Medallion Architecture**
PadrÃ£o de arquitetura de Data Lake/Warehouse que organiza dados em camadas incrementais: Bronze (raw), Silver (curated), Gold (aggregated).

**Bronze Layer (Camada Bronze)**
Primeira camada que armazena dados brutos com transformaÃ§Ãµes mÃ­nimas. Objetivo Ã© preservar dados de origem para auditoria.

**Silver Layer (Camada Silver)**
Segunda camada que armazena dados limpos, transformados e modelados. Otimizada para anÃ¡lises e consultas.

**Gold Layer (Camada Gold)**
Terceira camada (nÃ£o implementada neste projeto) que contÃ©m agregaÃ§Ãµes, KPIs e mÃ©tricas de negÃ³cio prÃ©-calculadas.

### Conceitos de Modelagem

**Star Schema (Esquema Estrela)**
Modelo dimensional onde uma tabela fato central Ã© conectada a mÃºltiplas tabelas dimensÃ£o, formando uma estrela.

**Fact Table (Tabela Fato)**
Tabela central do Star Schema que contÃ©m mÃ©tricas numÃ©ricas (fatos) e chaves estrangeiras para dimensÃµes.

**Dimension Table (Tabela DimensÃ£o)**
Tabela que contÃ©m atributos descritivos e contextuais para anÃ¡lise (quem, quando, onde, como).

**Surrogate Key (Chave Substituta)**
Chave primÃ¡ria artificial (geralmente autoincrement integer) que substitui chaves naturais. Prefixo: `sk_`.

**Natural Key (Chave Natural)**
Chave que existe naturalmente nos dados de negÃ³cio (CNPJ, CPF, email). Prefixo: `nk_`.

**Foreign Key (Chave Estrangeira)**
Coluna que referencia a chave primÃ¡ria de outra tabela. Prefixo: `sk_` (pois referenciam surrogate keys).

### Conceitos de ETL

**ETL (Extract, Transform, Load)**
Processo de extrair dados de origens, transformÃ¡-los conforme regras de negÃ³cio e carregÃ¡-los em destino.

**Ingestor**
Script responsÃ¡vel por ler dados de origem (CSV) e carregÃ¡-los na camada Bronze.

**Transformer (Transformador)**
Script responsÃ¡vel por transformar dados da Bronze para Silver aplicando regras de negÃ³cio.

**Template Method**
PadrÃ£o de design que define o esqueleto de um algoritmo em uma classe base, permitindo que subclasses implementem etapas especÃ­ficas.

**Context Manager**
PadrÃ£o Python que garante setup e cleanup de recursos (conexÃµes, arquivos) usando `with` statement.

**TRUNCATE/RELOAD**
EstratÃ©gia de carga que remove todos os registros da tabela e insere novamente. Usado em Bronze.

**FULL Load**
EstratÃ©gia de carga que substitui completamente os dados da tabela de destino.

**Incremental Load**
EstratÃ©gia de carga que adiciona apenas registros novos ou modificados.

### Conceitos de SCD

**SCD (Slowly Changing Dimension)**
TÃ©cnica para rastrear mudanÃ§as em dimensÃµes ao longo do tempo.

**SCD Type 1**
Sobrescreve dados antigos com novos. NÃ£o mantÃ©m histÃ³rico.

**SCD Type 2**
MantÃ©m histÃ³rico criando novos registros para cada mudanÃ§a. Usa campos de controle:
- `data_inicio` / `data_fim`: PerÃ­odo de validade
- `flag_ativo`: Indica versÃ£o atual (TRUE) ou histÃ³rica (FALSE)
- `versao`: NÃºmero sequencial da versÃ£o
- `hash_registro`: MD5 dos dados para detectar mudanÃ§as

**SCD Type 3**
MantÃ©m apenas versÃ£o anterior em colunas separadas (ex: `status_atual`, `status_anterior`).

### Conceitos de Qualidade

**Data Quality (Qualidade de Dados)**
Grau em que os dados atendem requisitos de completude, consistÃªncia, precisÃ£o e integridade.

**Dirty Data (Dados PoluÃ­dos)**
Dados com problemas de qualidade: valores NULL, duplicatas, formatos invÃ¡lidos, valores fora do range esperado.

**Referential Integrity (Integridade Referencial)**
Garantia de que chaves estrangeiras referenciam registros existentes nas tabelas dimensÃ£o.

**Hash MD5**
Algoritmo que gera uma string Ãºnica de 32 caracteres a partir de dados. Usado para detectar mudanÃ§as em SCD Type 2.

### Conceitos de Banco de Dados

**Schema (Esquema)**
Namespace que agrupa tabelas relacionadas. Exemplos: `bronze`, `silver`, `credits`.

**TRUNCATE**
Comando SQL que remove todos os registros de uma tabela (mais rÃ¡pido que DELETE).

**CASCADE**
OpÃ§Ã£o que propaga operaÃ§Ãµes para tabelas relacionadas (ex: TRUNCATE CASCADE remove dados de tabelas filhas).

**Index (Ãndice)**
Estrutura de dados que acelera consultas criando uma "tabela de busca" para uma ou mais colunas.

**Constraint (RestriÃ§Ã£o)**
Regra aplicada a colunas (PRIMARY KEY, FOREIGN KEY, NOT NULL, UNIQUE).

### Conceitos de Python

**DataFrame**
Estrutura de dados tabular do pandas (biblioteca Python) usada para manipular dados.

**Dataclass**
Classe Python que automatiza criaÃ§Ã£o de `__init__`, `__repr__` e outros mÃ©todos. Usado em `utils/config.py`.

**Type Hints**
AnotaÃ§Ãµes de tipo em Python (ex: `def funcao(x: int) -> str`) que melhoram legibilidade e permitem validaÃ§Ã£o.

**Context Manager**
Objeto que implementa `__enter__` e `__exit__` para gerenciar recursos. Usado com `with`.

### Ferramentas e Tecnologias

**Docker**
Plataforma de conteinerizaÃ§Ã£o que empacota aplicaÃ§Ãµes e dependÃªncias em ambientes isolados.

**Docker Compose**
Ferramenta para definir e executar aplicaÃ§Ãµes Docker multi-container usando arquivo YAML.

**PostgreSQL**
Sistema de gerenciamento de banco de dados relacional open-source.

**Loguru**
Biblioteca Python de logging com recursos avanÃ§ados (cores, rotaÃ§Ã£o, compressÃ£o).

**Tenacity**
Biblioteca Python para implementar retry logic com exponential backoff.

**pandas**
Biblioteca Python para anÃ¡lise e manipulaÃ§Ã£o de dados tabulares.

**psycopg2**
Driver PostgreSQL para Python.

---

## Perguntas Frequentes (FAQ)

**Q: Posso executar os scripts fora do Docker?**
R: Sim, mas vocÃª precisa instalar Python 3.10+, criar um virtualenv e instalar dependÃªncias via `pip install -r requirements.txt`. TambÃ©m precisa ajustar os paths de arquivos (remover `/app/`).

**Q: Como adicionar uma nova coluna a uma tabela existente?**
R: 1) Adicionar coluna no banco via `ALTER TABLE`, 2) Atualizar `get_column_mapping()` no ingestor, 3) Atualizar `get_bronze_columns()`, 4) Atualizar transformador Silver para processar nova coluna.

**Q: Ã‰ possÃ­vel reprocessar apenas um dia de dados?**
R: Atualmente nÃ£o, pois Bronze usa TRUNCATE/RELOAD. Para processar incrementalmente, vocÃª precisaria mudar a estratÃ©gia para INSERT com validaÃ§Ã£o de duplicatas.

**Q: Como conectar o Power BI ao Data Warehouse?**
R: Use o conector PostgreSQL do Power BI. Configure: Host (Azure), Port (5432), Database (creditsdw), UsuÃ¡rio, Senha. Aponte para o schema `silver`.

**Q: Os logs sÃ£o persistentes?**
R: Sim, logs ficam na pasta `logs/` que estÃ¡ montada como volume Docker. SÃ£o rotacionados a cada 100MB e mantidos por 30 dias.

**Q: Posso executar transformaÃ§Ãµes em paralelo?**
R: NÃ£o Ã© recomendado pois fact_faturamento depende das dimensÃµes. Execute na ordem: dim_clientes, dim_usuarios, fact_faturamento.

**Q: Como fazer backup dos dados?**
R: Use `pg_dump` para backup do banco PostgreSQL. Exemplo:
```bash
pg_dump -h creditsdw.postgres.database.azure.com -U creditsdw -d creditsdw -n bronze -n silver > backup.sql
```

---

## Contatos e Suporte

Para dÃºvidas, problemas ou sugestÃµes relacionadas ao Data Warehouse:

**Equipe de Engenharia de Dados - Credits Brasil**

- RepositÃ³rio GitHub: [github.com/brunocredits/credits-dw](https://github.com/brunocredits/credits-dw)
- Abra uma Issue para reportar bugs ou solicitar features

---

**Ãšltima atualizaÃ§Ã£o:** Novembro 2025
**VersÃ£o da documentaÃ§Ã£o:** 3.0
**Mantenedor:** Equipe de Engenharia de Dados
