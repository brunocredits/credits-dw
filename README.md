# ğŸ¦ Data Warehouse Credits Brasil

> **VersÃ£o:** 2.0 | **Arquitetura:** Bronze Layer | **PostgreSQL** 15

---

## ğŸ“‹ VisÃ£o Geral

Esta Ã© uma soluÃ§Ã£o de Data Warehouse para consolidar dados de diversas fontes em um banco de dados PostgreSQL. O projeto utiliza um pipeline de ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga) para processar arquivos CSV e carregÃ¡-los em uma camada **Bronze**, garantindo que os dados brutos sejam armazenados com o mÃ­nimo de transformaÃ§Ã£o.

O ambiente Ã© totalmente orquestrado com Docker, garantindo consistÃªncia e facilidade de uso.

### âœ¨ Recursos Principais

- âœ… **Camada Bronze**: Armazena dados brutos de fontes CSV.
- âœ… **Scripts de IngestÃ£o em Python**: Para um ETL robusto e modular.
- âœ… **OrquestraÃ§Ã£o com Docker**: Ambiente de desenvolvimento e produÃ§Ã£o consistente.
- âœ… **Auditoria de ExecuÃ§Ã£o**: Rastreia o status de cada ingestÃ£o no schema `credits`.

---

## ğŸ—ï¸ Arquitetura de Dados

A arquitetura de dados Ã© focada na simplicidade e robustez, com uma clara separaÃ§Ã£o de responsabilidades.

```
FONTES (Arquivos CSV) â†’ CAMADA BRONZE (Dados Brutos)
```

### Schemas do Banco de Dados

-   **`bronze`**: ContÃ©m os dados brutos exatamente como vÃªm das fontes, com o mÃ­nimo de processamento (ex: renomear colunas). Ã‰ a nossa fonte Ãºnica da verdade para os dados originais.
-   **`credits`**: Schema de metadados, usado para auditoria e controle do prÃ³prio processo de ETL. A tabela `historico_atualizacoes` registra cada execuÃ§Ã£o dos scripts, seu status, duraÃ§Ã£o e volume de dados.

### A Tabela de DimensÃ£o de Data (`bronze.data`)

Um destaque da nossa modelagem Ã© a tabela `bronze.data`. Embora pareÃ§a redundante Ã  primeira vista, ela Ã© uma ferramenta poderosa de anÃ¡lise conhecida como **Tabela de DimensÃ£o de Data**.

-   **Como funciona?** Para cada dia do calendÃ¡rio, armazenamos a data completa (`data_completa`) e tambÃ©m vÃ¡rios atributos prÃ©-calculados como `semestre`, `trimestre`, `mes` e `ano`.
-   **Por que usar?**
    1.  **Performance:** Consultas que agregam dados por perÃ­odos (ex: receita por trimestre) se tornam extremamente rÃ¡pidas, pois o banco de dados nÃ£o precisa calcular a qual trimestre uma data pertence para milhÃµes de registros; ele simplesmente usa o valor que jÃ¡ estÃ¡ armazenado.
    2.  **Simplicidade:** As consultas SQL ficam mais limpas e fÃ¡ceis de ler (`GROUP BY semestre` em vez de usar funÃ§Ãµes de data complexas).
    3.  **ConsistÃªncia:** Garante que todos na empresa usem a mesma definiÃ§Ã£o para perÃ­odos de tempo, evitando inconsistÃªncias em relatÃ³rios.

---

## ğŸ“‚ Estrutura do Projeto

```
credits-dw/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile             # Define a imagem do container de ETL
â”‚   â”œâ”€â”€ docker-compose.yml     # Orquestra os serviÃ§os
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ templates/         # ContÃ©m arquivos CSV de EXEMPLO com cabeÃ§alhos
â”‚       â””â”€â”€ input/             # Onde os arquivos a serem processados devem estar
â”‚           â””â”€â”€ onedrive/
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ ingestors/             # Scripts de ingestÃ£o por fonte (ex: csv)
â”‚   â””â”€â”€ utils/                 # MÃ³dulos de utilidade (conexÃ£o, log, etc.)
â”œâ”€â”€ .env                       # Arquivo de configuraÃ§Ã£o local (NÃƒO VERSIONADO)
â”œâ”€â”€ README.md                  # Esta documentaÃ§Ã£o
â””â”€â”€ requirements.txt           # DependÃªncias Python
```

---

## ğŸš€ InstalaÃ§Ã£o e Uso

### PrÃ©-requisitos

-   Docker e Docker Compose V2 (comando `docker compose`)
-   Python 3.10+ (para desenvolvimento local)
-   Um cliente PostgreSQL (ex: DBeaver, pgAdmin) para se conectar ao banco.

### 1. Configurar o Ambiente

1.  **Clone o repositÃ³rio:**
    ```bash
    git clone <URL_DO_REPOSITORIO>
    cd credits-dw
    ```

2.  **Crie o arquivo de ambiente:**
    Na raiz do projeto, crie um arquivo chamado `.env`. Ele guardarÃ¡ suas credenciais de banco de dados de forma segura. Copie o conteÃºdo abaixo e preencha com seus dados.

    ```properties
    # Credenciais do Banco de Dados PostgreSQL
    DB_HOST=<seu_host>
    DB_PORT=<sua_porta>
    DB_NAME=<seu_banco>
    DB_USER=<seu_usuario>
    DB_PASSWORD=<sua_senha>
    ```
    > **SeguranÃ§a:** O arquivo `.env` jÃ¡ estÃ¡ no `.gitignore`, garantindo que suas credenciais nunca sejam enviadas para o repositÃ³rio.

### 2. Preparar os Dados de Exemplo

Os scripts de ingestÃ£o procuram por arquivos CSV no diretÃ³rio `docker/data/input/onedrive/`. Para testar o pipeline, copie os arquivos de exemplo do diretÃ³rio `templates`:

```bash
# Copia os arquivos de exemplo para o local de ingestÃ£o
cp docker/data/templates/*.csv docker/data/input/onedrive/
```

### 3. Iniciar o Ambiente Docker

Todos os comandos devem ser executados a partir da raiz do projeto.

1.  **Construir e iniciar o container de ETL:**
    Este comando iniciarÃ¡ o serviÃ§o `etl-processor` em segundo plano. O container ficarÃ¡ ativo, pronto para executar os scripts.

    ```bash
    # Navegue atÃ© o diretÃ³rio docker e suba o container
    cd docker && docker compose up -d --build
    ```

2.  **Executar os Scripts de IngestÃ£o:**
    Para executar um script, use o comando `docker compose exec`.

    ```bash
    # Para executar TODOS os ingestores de CSV de uma vez
    docker compose exec etl-processor python python/run_all_ingestors.py

    # Para executar um ingestor especÃ­fico (ex: faturamento)
    docker compose exec etl-processor python python/ingestors/csv/ingest_faturamento.py
    ```

3.  **Parar o Ambiente:**
    Quando terminar, vocÃª pode parar e remover os containers.

    ```bash
    docker compose down
    ```

---

## ğŸ› ï¸ Desenvolvimento

### Acessando o Container

Para depurar ou executar comandos manualmente dentro do container:

```bash
docker compose exec etl-processor bash
```

### Qualidade de CÃ³digo

O projeto usa as seguintes ferramentas para garantir a qualidade do cÃ³digo:

```bash
# FormataÃ§Ã£o
black python/

# Linting
ruff check .

# Checagem de tipos
mypy python/
```
