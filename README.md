# Credits DW - Data Warehouse Pipeline

Pipeline de ingestÃ£o de dados para a camada Bronze do Data Warehouse, implementado com arquitetura modular e otimizado para alta performance.

## ğŸ—ï¸ Arquitetura

### Camadas do Data Warehouse
- **Bronze (RAW)**: Dados brutos validados e limpos
- **Silver**: Dados transformados e enriquecidos *(prÃ³xima etapa)*
- **Gold**: Dados agregados para consumo *(prÃ³xima etapa)*

### Componentes Principais

```
python/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ base_ingestor.py    # Orquestrador principal
â”‚   â”œâ”€â”€ data_cleaner.py     # Limpeza de dados
â”‚   â”œâ”€â”€ file_handler.py     # Gerenciamento de arquivos
â”‚   â””â”€â”€ validator.py        # ValidaÃ§Ã£o de estrutura
â”œâ”€â”€ ingestors/
â”‚   â”œâ”€â”€ ingest_faturamento.py
â”‚   â”œâ”€â”€ ingest_usuarios.py
â”‚   â””â”€â”€ ingest_base_oficial.py
â””â”€â”€ utils/
    â”œâ”€â”€ db_connection.py    # ConexÃ£o com PostgreSQL
    â””â”€â”€ audit.py            # Sistema de auditoria
```

## ğŸš€ Quick Start

### PrÃ©-requisitos
- Docker e Docker Compose
- PostgreSQL (Azure Database for PostgreSQL)
- Python 3.9+

### ConfiguraÃ§Ã£o

1. **Clone o repositÃ³rio**
```bash
git clone https://github.com/brunocredits/credits-dw.git
cd credits-dw
```

2. **Configure variÃ¡veis de ambiente**
```bash
cp .env.example .env
# Edite .env com suas credenciais
```

3. **Execute o pipeline**
```bash
./run_pipeline.sh
```

### Reset do Ambiente
Para limpar dados e preparar nova carga:
```bash
./reset_env.sh
```

## ğŸ“Š Funcionalidades

### âœ… Implementado

- **DetecÃ§Ã£o de Duplicatas**: Hash MD5 para evitar reprocessamento
- **ValidaÃ§Ã£o de Headers**: ComparaÃ§Ã£o com templates oficiais
- **Limpeza de Dados**:
  - ConversÃ£o de formato brasileiro (1.000,00 â†’ 1000.00)
  - Tratamento de hÃ­fen como zero
  - ValidaÃ§Ã£o de datas (DD/MM/YYYY)
- **IdempotÃªncia**: Delete-before-load por arquivo
- **Auditoria Completa**: Rastreamento de execuÃ§Ãµes e erros
- **Alta Performance**: PostgreSQL COPY para carga em massa

### ğŸ¯ CaracterÃ­sticas TÃ©cnicas

- **Arquitetura Modular**: PrincÃ­pios SOLID (SRP, DIP)
- **Clean Code**: CÃ³digo documentado em portuguÃªs
- **SeguranÃ§a**: ValidaÃ§Ã£o rigorosa, parÃ¢metros bind SQL
- **Performance**: OperaÃ§Ãµes vetorizadas (Pandas/Numpy)

## ğŸ“ Estrutura de Dados

### Tabelas Bronze

| Tabela | Colunas | Registros | DescriÃ§Ã£o |
|--------|---------|-----------|------------|
| `bronze.faturamento` | 36 | 213.403 | Dados de faturamento e recebÃ­veis |
| `bronze.base_oficial` | 15 | 3.037 | Cadastro de clientes ativos |
| `bronze.usuarios` | 13 | 40 | Cadastro de consultores/vendedores |
| `bronze.data` | 17 | 4.018 | DimensÃ£o temporal |

### Ãndices de Performance

A camada bronze possui **17 Ã­ndices** (~11.6MB) para otimizar queries:

**faturamento (6 Ã­ndices)**
- `idx_faturamento_cnpj` - Join com base_oficial
- `idx_faturamento_vendedor` - Join com usuarios
- `idx_faturamento_data_fat` - Filtros temporais
- `idx_faturamento_empresa_data` - AnÃ¡lises por empresa/perÃ­odo
- `idx_faturamento_status` - Filtros por status
- `faturamento_pkey` - Chave primÃ¡ria

**base_oficial (5 Ã­ndices)**
- `idx_base_oficial_cnpj` (UNIQUE) - Chave natural, join com faturamento
- `idx_base_oficial_lider` - Join com usuarios
- `idx_base_oficial_responsavel` - Join com usuarios
- `idx_base_oficial_status` - Filtros por status
- `base_oficial_pkey` - Chave primÃ¡ria

**usuarios (5 Ã­ndices)**
- `idx_usuarios_consultor` (UNIQUE) - Chave natural
- `idx_usuarios_cargo` - Filtros
- `idx_usuarios_time` - Filtros
- `idx_usuarios_status` - Filtros
- `usuarios_pkey` - Chave primÃ¡ria

**data (1 Ã­ndice)**
- `data_pkey` - Chave primÃ¡ria (data)

### Views de Monitoramento
- `bronze.v_index_usage` - Monitoramento de uso e performance dos Ã­ndices

### Auditoria
- `auditoria.historico_execucao` - Log de execuÃ§Ãµes
- `auditoria.log_rejeicao` - Linhas rejeitadas com motivo

## ğŸ” Queries de Monitoramento

### Verificar Ãºltima execuÃ§Ã£o
```sql
SELECT script_nome, data_inicio, status, 
       linhas_processadas, linhas_inseridas, linhas_erro
FROM auditoria.historico_execucao
ORDER BY data_inicio DESC
LIMIT 10;
```

### Analisar rejeiÃ§Ãµes
```sql
SELECT tabela_destino, motivo_rejeicao, COUNT(*) as qtd
FROM auditoria.log_rejeicao
GROUP BY tabela_destino, motivo_rejeicao
ORDER BY qtd DESC;
```

### Verificar duplicatas detectadas
```sql
SELECT COUNT(*) as arquivos_duplicados
FROM auditoria.historico_execucao
WHERE status = 'sucesso' 
  AND file_hash IN (
    SELECT file_hash 
    FROM auditoria.historico_execucao 
    GROUP BY file_hash 
    HAVING COUNT(*) > 1
  );
```

### Monitorar uso dos Ã­ndices
```sql
-- Ver quais Ã­ndices estÃ£o sendo mais utilizados
SELECT * FROM bronze.v_index_usage;

-- Verificar tamanho dos Ã­ndices
SELECT 
    schemaname,
    tablename,
    indexname,
    pg_size_pretty(pg_relation_size(indexrelid)) as size
FROM pg_stat_user_indexes
WHERE schemaname = 'bronze'
ORDER BY pg_relation_size(indexrelid) DESC;
```

## ğŸ“‹ Roadmap de Desenvolvimento

### ğŸ”„ Fase 2: Camada Silver (TransformaÃ§Ã£o e Qualidade)

**Objetivo**: Dados limpos, deduplicados e enriquecidos prontos para anÃ¡lise

#### 2.1 Arquitetura Silver
```
silver/
â”œâ”€â”€ dim_clientes          # DimensÃ£o de clientes (SCD Type 2)
â”œâ”€â”€ dim_usuarios          # DimensÃ£o de usuÃ¡rios
â”œâ”€â”€ dim_tempo             # DimensÃ£o temporal (jÃ¡ existe em bronze.data)
â”œâ”€â”€ fato_faturamento      # Fatos de faturamento transformados
â””â”€â”€ metricas_qualidade    # MÃ©tricas de qualidade de dados
```

#### 2.2 TransformaÃ§Ãµes Planejadas

**DeduplicaÃ§Ã£o Inteligente**
- [ ] Implementar algoritmo de matching fuzzy para clientes
- [ ] Criar regras de merge baseadas em:
  - CNPJ (chave primÃ¡ria)
  - RazÃ£o social (similaridade > 85%)
  - EndereÃ§o e telefone (dados auxiliares)
- [ ] Manter histÃ³rico de merges na auditoria

**Enriquecimento de Dados**
- [ ] Integrar API da Receita Federal (validaÃ§Ã£o CNPJ)
- [ ] Adicionar geolocalizaÃ§Ã£o (CEP â†’ lat/long)
- [ ] Calcular mÃ©tricas derivadas:
  - Aging de recebÃ­veis (dias em atraso)
  - Score de inadimplÃªncia
  - Ticket mÃ©dio por cliente
  - Lifetime Value (LTV)

**Slowly Changing Dimensions (SCD Type 2)**
- [ ] Implementar versionamento de clientes
- [ ] Campos de controle:
  - `valid_from` - Data inÃ­cio vigÃªncia
  - `valid_to` - Data fim vigÃªncia
  - `is_current` - Flag de versÃ£o atual
  - `version` - NÃºmero da versÃ£o
- [ ] Trigger automÃ¡tico para criar nova versÃ£o em mudanÃ§as

**ValidaÃ§Ãµes de NegÃ³cio**
- [ ] Regras de consistÃªncia:
  - Valor a receber > 0
  - Data vencimento >= Data faturamento
  - Cliente existe na base oficial
  - Vendedor ativo no sistema
- [ ] Quarentena para dados suspeitos
- [ ] Alertas automÃ¡ticos para anomalias

#### 2.3 Cronograma Silver (Estimativa: 3-4 semanas)

**Semana 1**: Estrutura e DeduplicaÃ§Ã£o
- Criar schema `silver` no banco
- Implementar `SilverTransformer` base
- Desenvolver algoritmo de deduplicaÃ§Ã£o
- Testes unitÃ¡rios de matching

**Semana 2**: Enriquecimento
- Integrar APIs externas (Receita Federal)
- Implementar cÃ¡lculo de mÃ©tricas derivadas
- Adicionar geolocalizaÃ§Ã£o
- Criar pipeline de enriquecimento

**Semana 3**: SCD Type 2
- Implementar versionamento de dimensÃµes
- Criar triggers de atualizaÃ§Ã£o
- Desenvolver queries de consulta histÃ³rica
- Testes de integridade temporal

**Semana 4**: ValidaÃ§Ãµes e Qualidade
- Implementar regras de negÃ³cio
- Criar sistema de quarentena
- Desenvolver dashboard de qualidade
- DocumentaÃ§Ã£o e testes de integraÃ§Ã£o

---

### ğŸ“Š Fase 3: Camada Gold (Analytics e BI)

**Objetivo**: Dados agregados e otimizados para consumo em dashboards e relatÃ³rios

#### 3.1 Arquitetura Gold
```
gold/
â”œâ”€â”€ fato_faturamento_mensal    # AgregaÃ§Ã£o mensal
â”œâ”€â”€ fato_faturamento_diario    # AgregaÃ§Ã£o diÃ¡ria
â”œâ”€â”€ metricas_vendedores        # Performance de vendedores
â”œâ”€â”€ metricas_clientes          # AnÃ¡lise de clientes
â”œâ”€â”€ metricas_produtos          # AnÃ¡lise de produtos/serviÃ§os
â””â”€â”€ kpis_executivos            # KPIs consolidados
```

#### 3.2 MÃ©tricas e KPIs Planejados

**Faturamento**
- [ ] Receita total (MRR - Monthly Recurring Revenue)
- [ ] Receita por canal de vendas
- [ ] Receita por regiÃ£o geogrÃ¡fica
- [ ] Taxa de crescimento (MoM, YoY)
- [ ] Forecast de recebimento (prÃ³ximos 30/60/90 dias)

**InadimplÃªncia**
- [ ] Taxa de inadimplÃªncia (%)
- [ ] Valor em atraso por faixa (0-30, 31-60, 61-90, 90+ dias)
- [ ] Top 10 clientes inadimplentes
- [ ] ProvisÃ£o para devedores duvidosos (PDD)

**Performance de Vendedores**
- [ ] Ranking de vendedores (por volume e valor)
- [ ] Taxa de conversÃ£o
- [ ] Ticket mÃ©dio por vendedor
- [ ] Churn de clientes por vendedor

**AnÃ¡lise de Clientes**
- [ ] SegmentaÃ§Ã£o RFM (Recency, Frequency, Monetary)
- [ ] Customer Lifetime Value (CLV)
- [ ] Taxa de retenÃ§Ã£o/churn
- [ ] Net Promoter Score (NPS) - se disponÃ­vel

**AnÃ¡lise Temporal**
- [ ] Sazonalidade de vendas
- [ ] TendÃªncias de crescimento
- [ ] PrevisÃ£o de demanda (ML)

#### 3.3 Views Materializadas

**Refresh AutomÃ¡tico**
```sql
-- Exemplo: AtualizaÃ§Ã£o incremental diÃ¡ria
CREATE MATERIALIZED VIEW gold.fato_faturamento_diario AS
SELECT 
    d.data,
    COUNT(DISTINCT f.cliente_id) as clientes_ativos,
    SUM(f.valor_a_receber) as receita_total,
    AVG(f.valor_a_receber) as ticket_medio,
    COUNT(*) as num_transacoes
FROM silver.fato_faturamento f
JOIN silver.dim_tempo d ON f.data_faturamento = d.data
GROUP BY d.data;

-- Refresh diÃ¡rio Ã s 2h da manhÃ£
CREATE INDEX idx_gold_fat_diario_data ON gold.fato_faturamento_diario(data);
REFRESH MATERIALIZED VIEW CONCURRENTLY gold.fato_faturamento_diario;
```

#### 3.4 OtimizaÃ§Ãµes de Performance

**Particionamento**
- [ ] Particionar tabelas por data (mensal)
- [ ] Implementar partition pruning
- [ ] Configurar auto-vacuum por partiÃ§Ã£o

**Ãndices EstratÃ©gicos**
- [ ] Ãndices compostos para queries frequentes
- [ ] Ãndices parciais para filtros comuns
- [ ] BRIN indexes para colunas temporais

**AgregaÃ§Ãµes PrÃ©-calculadas**
- [ ] Cubos OLAP para anÃ¡lise multidimensional
- [ ] Rollup tables para diferentes granularidades
- [ ] Cache de queries complexas (Redis)

#### 3.5 Cronograma Gold (Estimativa: 4-5 semanas)

**Semana 1**: Estrutura Base
- Criar schema `gold` e tabelas de fatos
- Implementar agregaÃ§Ãµes bÃ¡sicas (diÃ¡rio/mensal)
- Desenvolver `GoldAggregator` base
- Testes de performance iniciais

**Semana 2**: MÃ©tricas de NegÃ³cio
- Implementar KPIs de faturamento
- Desenvolver mÃ©tricas de inadimplÃªncia
- Criar anÃ¡lises de vendedores
- Dashboard de mÃ©tricas em tempo real

**Semana 3**: Analytics AvanÃ§ado
- Implementar segmentaÃ§Ã£o RFM
- Desenvolver anÃ¡lise de cohort
- Criar previsÃµes com ML (Prophet/ARIMA)
- AnÃ¡lise de sazonalidade

**Semana 4**: OtimizaÃ§Ã£o
- Implementar particionamento
- Criar Ã­ndices otimizados
- Configurar views materializadas
- Testes de carga e performance

**Semana 5**: IntegraÃ§Ã£o BI
- Conectar Power BI / Metabase
- Criar dashboards executivos
- Desenvolver relatÃ³rios automatizados
- DocumentaÃ§Ã£o de uso

---

### ğŸ”§ Melhorias TÃ©cnicas Paralelas

**Testes e Qualidade**
- [ ] Cobertura de testes > 80%
- [ ] Testes de carga (Apache JMeter)
- [ ] Testes de regressÃ£o automatizados

**DevOps e Infraestrutura**
- [ ] CI/CD com GitHub Actions
- [ ] Deploy automatizado (staging â†’ prod)
- [ ] Rollback automÃ¡tico em falhas
- [ ] Blue-green deployment

**Observabilidade**
- [ ] Logs estruturados (JSON)
- [ ] MÃ©tricas customizadas (Prometheus)
- [ ] Dashboards de monitoramento (Grafana)
- [ ] Alertas inteligentes (PagerDuty)

**SeguranÃ§a**
- [ ] Criptografia de dados em repouso
- [ ] Auditoria de acessos (quem viu o quÃª)
- [ ] Mascaramento de dados sensÃ­veis
- [ ] Compliance LGPD

### ğŸ” SeguranÃ§a
- [ ] Implementar criptografia de dados sensÃ­veis
- [ ] Adicionar auditoria de acessos
- [ ] Configurar backup automÃ¡tico
- [ ] Implementar disaster recovery

### ğŸ“± Observabilidade
- [ ] Dashboard de mÃ©tricas em tempo real
- [ ] Logs centralizados (ELK Stack)
- [ ] Rastreamento distribuÃ­do (OpenTelemetry)
- [ ] SLA monitoring

## ğŸ› ï¸ Desenvolvimento

### Estrutura de Branches
- `main` - ProduÃ§Ã£o
- `develop` - Desenvolvimento
- `feature/*` - Novas funcionalidades
- `hotfix/*` - CorreÃ§Ãµes urgentes

### PadrÃµes de Commit
```
feat: Nova funcionalidade
fix: CorreÃ§Ã£o de bug
docs: DocumentaÃ§Ã£o
refactor: RefatoraÃ§Ã£o
perf: Melhoria de performance
test: Testes
chore: Tarefas gerais
```

## ğŸ“š DocumentaÃ§Ã£o Adicional

- [Guia de DemonstraÃ§Ã£o](/.gemini/antigravity/brain/.../DEMO_GUIDE.md)
- [Regras de ValidaÃ§Ã£o](/.gemini/antigravity/brain/.../regras_validacao_faturamento.md)
- [Estrutura do Banco](/.gemini/antigravity/brain/.../estrutura_banco_dados.md)

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'feat: Add AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“ LicenÃ§a

Projeto proprietÃ¡rio - Credits Brasil

## ğŸ‘¥ Time

- **Desenvolvedor**: Bruno Pires
- **OrganizaÃ§Ã£o**: Credits Brasil

---

**Status**: âœ… Bronze Layer - ProduÃ§Ã£o  
**PrÃ³xima Milestone**: ğŸ”„ Silver Layer - Em Planejamento