# Credits Brasil Data Warehouse (Credits DW)

Este projeto implementa um pipeline de dados (ETL) containerizado para ingerir dados brutos de arquivos (CSV, Excel, ODS) em um Data Warehouse PostgreSQL na camada Bronze (Raw).

## ğŸ“‹ VisÃ£o Geral

O pipeline Ã© desenvolvido em Python e orquestrado via Docker Compose. Ele suporta:
- **IngestÃ£o DinÃ¢mica:** Detecta automaticamente arquivos de `faturamento`, `base_oficial` e `usuarios` no diretÃ³rio de input.
- **ValidaÃ§Ã£o de Schema:** Verifica se os arquivos de entrada correspondem aos templates esperados.
- **Limpeza de Dados:** Tratamento bÃ¡sico de tipos numÃ©ricos e datas.
- **Auditoria Robusta:** Logs de execuÃ§Ã£o e tabela de rejeiÃ§Ã£o (`auditoria.log_rejeicao`) detalhada no banco de dados.
- **EstratÃ©gia "Warn-on-Fail":** Registros com campos obrigatÃ³rios vazios sÃ£o ingeridos com um aviso (WARN), enquanto erros crÃ­ticos de dados rejeitam o registro (ERROR).

## ğŸ—ï¸ Estrutura do Projeto

```
credits-dw/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ input/       # Coloque seus arquivos CSV/XLSX aqui
â”‚   â”‚   â”œâ”€â”€ processed/   # Arquivos processados sÃ£o movidos para cÃ¡
â”‚   â”‚   â””â”€â”€ templates/   # Templates para validaÃ§Ã£o de cabeÃ§alho
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ core/            # LÃ³gica base (Ingestor, Validador, Cleaner)
â”‚   â”œâ”€â”€ ingestors/       # Classes especÃ­ficas para cada tipo de arquivo
â”‚   â”œâ”€â”€ scripts/         # Scripts executÃ¡veis (run_pipeline.py)
â”‚   â””â”€â”€ utils/           # UtilitÃ¡rios (DB, Logger)
â”œâ”€â”€ logs/                # Logs de execuÃ§Ã£o em arquivo
â”œâ”€â”€ QUERIES.md           # Exemplos de consultas SQL
â”œâ”€â”€ run_pipeline.sh      # Script facilitador para rodar o ETL
â”œâ”€â”€ reset_env.sh         # Script para limpar dados e resetar tabelas
â””â”€â”€ requirements.txt
```

## ğŸš€ Como Executar

### PrÃ©-requisitos
- Docker e Docker Compose instalados.
- Arquivo `.env` configurado (copie de `.env.example`).

### 1. Configurar VariÃ¡veis de Ambiente
Crie um arquivo `.env` na raiz do projeto com as credenciais do banco:
```bash
cp .env.example .env
# Edite o .env com suas configuraÃ§Ãµes
```

### 2. Colocar Arquivos de Input
Mova os arquivos que deseja processar para `docker/data/input/`.
Exemplo:
```bash
cp meus_dados/*.csv docker/data/input/
```

### 3. Rodar o Pipeline
Execute o script wrapper:
```bash
./run_pipeline.sh
```
Isso irÃ¡:
1. Buildar a imagem Docker.
2. Executar o processamento.
3. Mover os arquivos processados para `docker/data/processed/YYYY/MM/DD/`.
4. Registrar o resultado no banco de dados.

### 4. Verificar Resultados
Consulte o arquivo [QUERIES.md](QUERIES.md) para exemplos de como explorar os dados ingeridos.

Para verificar erros ou avisos de ingestÃ£o:
```sql
SELECT * FROM auditoria.log_rejeicao ORDER BY data_hora DESC LIMIT 100;
```

## ğŸ› ï¸ Comandos Ãšteis

- **Resetar Ambiente:** Limpa tabelas bronze e arquivos processados (CUIDADO!).
  ```bash
  ./reset_env.sh
  ```

- **Logs:** Verifique a pasta `logs/` para detalhes tÃ©cnicos da execuÃ§Ã£o.

## ğŸ“ DecisÃµes de Arquitetura

- **Camada Bronze (Raw):** O foco Ã© ingerir os dados com mÃ­nima transformaÃ§Ã£o destrutiva.
- **ValidaÃ§Ã£o FlexÃ­vel:** Campos obrigatÃ³rios ausentes geram alertas (`WARN`) mas nÃ£o bloqueiam a ingestÃ£o, permitindo correÃ§Ã£o posterior na camada Silver.
- **Alta Performance:** Uso de `COPY FROM STDIN` do PostgreSQL para carga em massa.
