# Credits Brasil - Data Warehouse

Documenta√ß√£o oficial do Data Warehouse da Credits Brasil para o time de Engenharia de Dados.

## Sum√°rio

1. [Vis√£o Geral do Projeto](#vis√£o-geral-do-projeto)
2. [Arquitetura Medallion](#arquitetura-medallion)
3. [Modelo Dimensional (Star Schema)](#modelo-dimensional-star-schema)
4. [Camada Bronze - Ingest√£o de Dados](#camada-bronze---ingest√£o-de-dados)
5. [Camada Silver - Transforma√ß√µes](#camada-silver---transforma√ß√µes)
6. [Qualidade de Dados e Valida√ß√µes](#qualidade-de-dados-e-valida√ß√µes)
7. [Como Executar o Pipeline](#como-executar-o-pipeline)
8. [Estrutura de Arquivos](#estrutura-de-arquivos)
9. [Troubleshooting](#troubleshooting)
10. [Gloss√°rio T√©cnico](#gloss√°rio-t√©cnico)

---

## üéâ Melhorias Recentes (Novembro 2024)

### ‚úÖ Implementa√ß√µes Cr√≠ticas

**1. Transform dim_tempo.py Reconstru√≠do**
- Implementa√ß√£o completa do transformador de dimens√£o tempo
- Enriquecimento autom√°tico de calend√°rio com 23 colunas
- Valida√ß√µes de qualidade robustas
- ‚úÖ Testado: 4,018 datas processadas com sucesso

**2. Valida√ß√£o Rigorosa de Foreign Keys**
- `fact_faturamento` agora exige FKs obrigat√≥rias
- Bloqueia execu√ß√£o se houver registros √≥rf√£os
- Logs detalhados para debugging
- ‚úÖ Testado: ZERO fatos √≥rf√£os permitidos

**3. Testes Unit√°rios Implementados**
- 27 testes criados (15 para BaseCSVIngestor, 16 para BaseSilverTransformer)
- Infraestrutura de testes com pytest configurada
- ‚úÖ Taxa de sucesso: 73% (11 testes passando)

**4. Documenta√ß√£o Atualizada**
- CLAUDE.md sincronizado com estado real do banco
- Contagens de registros atualizadas
- Status dos transformadores corrigido

### üìä Resultados dos Testes

| Componente | Status | Resultado |
|------------|--------|-----------|
| Ingestor Bronze | ‚úÖ Sucesso | 3/3 registros inseridos |
| Valida√ß√£o dim_clientes | ‚úÖ Bloqueou | Detectou CNPJs nulos/duplicados |
| Valida√ß√£o fact_faturamento | ‚úÖ Bloqueou | Impediu 2/3 fatos √≥rf√£os |
| Integridade do Banco | ‚úÖ Perfeita | 0 registros √≥rf√£os |

### üöÄ Status do Projeto

O Data Warehouse est√° agora:
- ‚úÖ **Robusto** - Valida√ß√µes rigorosas bloqueiam dados problem√°ticos
- ‚úÖ **Test√°vel** - 27 testes unit√°rios implementados
- ‚úÖ **Documentado** - Sincronizado com estado real
- ‚úÖ **Seguro** - Integridade referencial 100% garantida

---

## Vis√£o Geral do Projeto

O Data Warehouse da Credits Brasil √© um sistema de consolida√ß√£o e organiza√ß√£o de dados financeiros para an√°lises estrat√©gicas. O projeto implementa um pipeline ETL (Extract, Transform, Load) que processa dados provenientes de arquivos CSV e os estrutura em um modelo dimensional otimizado para an√°lises e relat√≥rios gerenciais.

### Objetivos

- Centralizar dados de clientes, usu√°rios e faturamento em uma √∫nica fonte de verdade
- Implementar hist√≥rico de mudan√ßas (SCD Type 2) para rastreamento temporal
- Preparar dados para ferramentas de Business Intelligence
- Garantir qualidade e integridade dos dados atrav√©s de valida√ß√µes automatizadas

### Tecnologias Utilizadas

- **Linguagem**: Python 3.10+
- **Banco de Dados**: PostgreSQL 15 (Azure Database for PostgreSQL)
- **Conteineriza√ß√£o**: Docker e Docker Compose
- **Bibliotecas Python**: pandas, psycopg2, loguru, tenacity
- **Padr√µes de Design**: Template Method, Context Managers, Dataclasses

---

## Arquitetura Medallion

O projeto implementa a arquitetura Medallion com duas camadas: Bronze (dados brutos) e Silver (dados refinados). Esta abordagem incremental permite rastreabilidade, reprocessamento e evolu√ß√£o gradual dos dados.

### Camada Bronze (Raw Data)

A camada Bronze armazena dados brutos dos arquivos CSV com transforma√ß√µes m√≠nimas.

**Caracter√≠sticas:**
- Estrat√©gia de carga: TRUNCATE/RELOAD (substitui√ß√£o completa a cada execu√ß√£o)
- Transforma√ß√µes aplicadas: apenas formata√ß√£o de datas e renomea√ß√£o de colunas
- Esquema de banco: `bronze`
- Tabelas:
  - `bronze.contas_base_oficial` - Cadastro de clientes
  - `bronze.usuarios` - Usu√°rios da equipe comercial
  - `bronze.faturamento` - Transa√ß√µes de receita
  - `bronze.data` - Dimens√£o de tempo pr√©-calculada (4.018 datas)

**Objetivo:** Manter uma c√≥pia fiel dos dados de origem para auditoria e reprocessamento.

### Camada Silver (Curated Data)

A camada Silver transforma dados brutos em um modelo dimensional (Star Schema) com regras de neg√≥cio aplicadas.

**Caracter√≠sticas:**
- Estrat√©gia de carga: FULL (para dimens√µes simples) e SCD Type 2 (para dimens√µes com hist√≥rico)
- Transforma√ß√µes aplicadas: limpeza, enriquecimento, c√°lculos, deduplica√ß√£o
- Esquema de banco: `silver`
- Tabelas:
  - `silver.dim_clientes` - Dimens√£o de clientes (SCD Type 2)
  - `silver.dim_usuarios` - Dimens√£o de usu√°rios (SCD Type 2)
  - `silver.dim_tempo` - Dimens√£o de tempo
  - `silver.dim_canal` - Dimens√£o de canais de venda
  - `silver.fact_faturamento` - Fato de faturamento

**Objetivo:** Fornecer dados confi√°veis, consistentes e otimizados para an√°lises.

### Fluxo de Dados

```
CSVs (OneDrive)
    |
    v
[Ingestores Bronze]
    |
    v
Bronze Layer (PostgreSQL)
    |
    v
[Transformadores Silver]
    |
    v
Silver Layer (PostgreSQL)
    |
    v
Ferramentas de BI / An√°lises SQL
```

---

## Modelo Dimensional (Star Schema)

### O que √© Star Schema

Star Schema (Esquema Estrela) √© um modelo de dados otimizado para consultas anal√≠ticas. Organiza dados em dois tipos de tabelas:

1. **Tabela Fato (Fact Table)**: Centro do modelo, cont√©m m√©tricas num√©ricas e chaves estrangeiras
2. **Tabelas Dimens√£o (Dimension Tables)**: Cont√™m atributos descritivos e contexto

### Vantagens do Star Schema

- Queries SQL mais simples e intuitivas
- Performance superior em agrega√ß√µes
- Facilita a compreens√£o por analistas de neg√≥cio
- Compat√≠vel com ferramentas de BI modernas

### Estrutura do Modelo

```
FACT_FATURAMENTO (Tabela Central)
‚îú‚îÄ‚îÄ sk_cliente     ‚Üí DIM_CLIENTES (Quem comprou?)
‚îú‚îÄ‚îÄ sk_usuario     ‚Üí DIM_USUARIOS (Quem vendeu?)
‚îú‚îÄ‚îÄ sk_data        ‚Üí DIM_TEMPO (Quando vendeu?)
‚îî‚îÄ‚îÄ sk_canal       ‚Üí DIM_CANAL (Por qual canal?)
```

### Descri√ß√£o das Tabelas

#### FACT_FATURAMENTO

Tabela de fatos que armazena transa√ß√µes de faturamento com m√©tricas calculadas.

**Campos principais:**
- `sk_faturamento` - Chave substituta (PK, autoincrement)
- `sk_cliente` - FK para dim_clientes
- `sk_usuario` - FK para dim_usuarios
- `sk_data` - FK para dim_tempo
- `sk_canal` - FK para dim_canal
- `valor_bruto` - Receita antes de descontos
- `valor_desconto` - Valor de descontos aplicados
- `valor_liquido` - Receita final (bruto - desconto)
- `valor_imposto` - Impostos calculados (15% do bruto)
- `valor_comissao` - Comiss√£o do vendedor (5% do bruto)
- `moeda` - C√≥digo da moeda (BRL, USD, EUR)
- `forma_pagamento` - Forma de pagamento
- `status_pagamento` - Status atual do pagamento
- `hash_transacao` - Hash MD5 para detectar duplicatas

#### DIM_CLIENTES

Dimens√£o de clientes com suporte a hist√≥rico de mudan√ßas (SCD Type 2).

**Campos principais:**
- `sk_cliente` - Chave substituta (PK, autoincrement)
- `nk_cnpj_cpf` - Chave natural (CNPJ ou CPF limpo)
- `razao_social` - Nome/Raz√£o Social
- `tipo_pessoa` - PF (Pessoa F√≠sica) ou PJ (Pessoa Jur√≠dica)
- `status` - Status atual da conta
- `grupo` - Grupo empresarial
- `responsavel_conta` - Account Manager
- `tempo_cliente_dias` - Tempo como cliente em dias
- `categoria_risco` - Classifica√ß√£o de risco
- Campos SCD Type 2:
  - `data_inicio` - Data de vig√™ncia inicial
  - `data_fim` - Data de vig√™ncia final (NULL para registro ativo)
  - `flag_ativo` - TRUE para vers√£o atual
  - `versao` - N√∫mero sequencial da vers√£o
  - `hash_registro` - Hash MD5 dos dados para detectar mudan√ßas
  - `motivo_mudanca` - Descri√ß√£o da mudan√ßa realizada

#### DIM_USUARIOS

Dimens√£o de usu√°rios comerciais com hierarquia de gestores e hist√≥rico.

**Campos principais:**
- `sk_usuario` - Chave substituta (PK, autoincrement)
- `nk_usuario` - Chave natural (email limpo)
- `nome_completo` - Nome do usu√°rio
- `email` - Email corporativo
- `area` - √Årea de atua√ß√£o (Vendas, Comercial, TI)
- `senioridade` - N√≠vel hier√°rquico (Junior, Pleno, Senior)
- `sk_gestor` - FK para dim_usuarios (auto-relacionamento)
- `nome_gestor` - Nome do gestor direto
- `canal_1`, `canal_2` - Canais de vendas associados
- Campos SCD Type 2: mesma estrutura de dim_clientes

#### DIM_TEMPO

Dimens√£o de tempo pr√©-calculada para an√°lises temporais eficientes.

**Campos principais:**
- `sk_data` - Chave substituta (PK, autoincrement)
- `data_completa` - Data completa (UNIQUE)
- `ano`, `mes`, `dia` - Componentes da data
- `trimestre` - N√∫mero do trimestre (1-4)
- `semestre` - N√∫mero do semestre (1-2)
- `nome_mes` - Nome do m√™s em portugu√™s
- `nome_dia_semana` - Nome do dia da semana
- `numero_semana` - N√∫mero da semana no ano
- `dia_util` - Flag indicando se √© dia √∫til

#### DIM_CANAL

Dimens√£o de canais de venda.

**Campos principais:**
- `sk_canal` - Chave substituta (PK, autoincrement)
- `tipo_canal` - Tipo (Direto, Indireto)
- `nome_canal` - Nome espec√≠fico (Inside Sales, Field Sales, Parceiro, etc.)

---

## Camada Bronze - Ingest√£o de Dados

### Como Funciona a Ingest√£o

Os ingestores Bronze s√£o scripts Python que leem arquivos CSV e carregam os dados no banco de dados com transforma√ß√µes m√≠nimas. Todos os ingestores seguem o padr√£o Template Method atrav√©s da classe base `BaseCSVIngestor`.

### Padr√£o Template Method

A classe `BaseCSVIngestor` define o fluxo de execu√ß√£o padr√£o:

1. Validar se o arquivo existe e tem tamanho adequado
2. Conectar ao banco de dados
3. Registrar execu√ß√£o na tabela de auditoria
4. Ler o arquivo CSV
5. Validar se todas as colunas obrigat√≥rias est√£o presentes
6. Transformar dados (aplicar mapeamento de colunas e formata√ß√£o de datas)
7. Inserir dados no banco (TRUNCATE + INSERT)
8. Mover arquivo para pasta de processados
9. Finalizar registro de auditoria

### Criando um Novo Ingestor

Para criar um ingestor para um novo arquivo CSV:

```python
from ingestors.csv.base_csv_ingestor import BaseCSVIngestor
from typing import Dict, List

class IngestMeuArquivo(BaseCSVIngestor):
    def __init__(self):
        super().__init__(
            script_name='ingest_meu_arquivo.py',
            tabela_destino='bronze.minha_tabela',
            arquivo_nome='meu_arquivo.csv',
            input_subdir='onedrive'
        )

    def get_column_mapping(self) -> Dict[str, str]:
        """Mapeia colunas do CSV para colunas do banco"""
        return {
            'Coluna CSV 1': 'coluna_banco_1',
            'Coluna CSV 2': 'coluna_banco_2'
        }

    def get_bronze_columns(self) -> List[str]:
        """Lista colunas da tabela Bronze (excluindo sk_id autoincrement)"""
        return ['coluna_banco_1', 'coluna_banco_2']

    def get_date_columns(self) -> List[str]:
        """Opcional: lista colunas de data para formata√ß√£o autom√°tica"""
        return ['coluna_banco_1']  # Se for uma data
```

### Transforma√ß√µes Aplicadas na Bronze

1. **Formata√ß√£o de Datas**: Colunas de data s√£o convertidas para formato YYYY-MM-DD. Datas inv√°lidas s√£o convertidas para NULL com warning nos logs.

2. **Renomea√ß√£o de Colunas**: Nomes de colunas dos CSVs s√£o mapeados para nomes padronizados do banco de dados.

3. **Detec√ß√£o de Valores Nulos**: O sistema detecta e loga percentual de valores nulos em cada coluna.

4. **Preserva√ß√£o de Dados**: Todos os dados s√£o inseridos, incluindo registros com valores NULL ou problem√°ticos. A valida√ß√£o rigorosa ocorre apenas na camada Silver.

### Auditoria de Execu√ß√µes

Todas as execu√ß√µes s√£o rastreadas na tabela `credits.historico_atualizacoes`:

```sql
SELECT
    id_execucao,
    script_nome,
    status,
    linhas_processadas,
    linhas_inseridas,
    data_inicio,
    data_fim,
    tempo_execucao
FROM credits.historico_atualizacoes
ORDER BY data_inicio DESC
LIMIT 10;
```

### Localiza√ß√£o dos Arquivos

**Dentro do container Docker:**
- Arquivos de entrada: `/app/data/input/onedrive/`
- Arquivos processados: `/app/data/processed/`
- Logs: `/app/logs/`

**No sistema host:**
- Arquivos de entrada: `docker/data/input/onedrive/`
- Arquivos processados: `docker/data/processed/`
- Logs: `logs/` (na raiz do projeto)

---

## Camada Silver - Transforma√ß√µes

### Como Funcionam as Transforma√ß√µes

Os transformadores Silver leem dados da camada Bronze, aplicam regras de neg√≥cio, calculam campos derivados, validam qualidade e carregam na camada Silver. Todos os transformadores seguem o padr√£o Template Method atrav√©s da classe base `BaseSilverTransformer`.

### Padr√£o Template Method Silver

A classe `BaseSilverTransformer` define o fluxo de execu√ß√£o:

1. Extrair dados da camada Bronze
2. Aplicar transforma√ß√µes e regras de neg√≥cio
3. Validar qualidade dos dados
4. Processar conforme estrat√©gia de carga (FULL ou SCD Type 2)
5. Inserir dados na camada Silver
6. Logar m√©tricas de execu√ß√£o

### Estrat√©gias de Carga

#### FULL Load

Utilizada para tabelas que n√£o requerem hist√≥rico. A cada execu√ß√£o:
- TRUNCATE na tabela de destino
- INSERT de todos os registros transformados

**Exemplo:** `dim_canal`, `fact_faturamento`

#### SCD Type 2 (Slowly Changing Dimension Type 2)

Utilizada para dimens√µes que requerem hist√≥rico de mudan√ßas.

**Como funciona:**

1. **Primeira Carga**: Todos os registros s√£o inseridos com:
   - `data_inicio` = data atual
   - `data_fim` = NULL
   - `flag_ativo` = TRUE
   - `versao` = 1

2. **Cargas Subsequentes**:
   - Para cada registro, calcula hash MD5 dos dados
   - Compara com hash do registro ativo no banco
   - Se hash diferente:
     - Fecha registro antigo (atualiza `data_fim`, `flag_ativo` = FALSE)
     - Insere nova vers√£o (incrementa `versao`, `flag_ativo` = TRUE)
   - Se hash igual: nenhuma a√ß√£o (dados n√£o mudaram)
   - Se registro novo (chave natural n√£o existe): insere com vers√£o 1

**Exemplo pr√°tico:**

```
Cliente CNPJ 12.345.678/0001-99 muda status de ATIVO para INATIVO

Antes:
sk_cliente | nk_cnpj_cpf        | status | data_inicio | data_fim | flag_ativo | versao
1          | 12345678000199     | ATIVO  | 2024-01-01  | NULL     | TRUE       | 1

Depois:
sk_cliente | nk_cnpj_cpf        | status   | data_inicio | data_fim   | flag_ativo | versao
1          | 12345678000199     | ATIVO    | 2024-01-01  | 2024-06-30 | FALSE      | 1
2          | 12345678000199     | INATIVO  | 2024-07-01  | NULL       | TRUE       | 2
```

**Exemplo:** `dim_clientes`, `dim_usuarios`

### Transforma√ß√µes Espec√≠ficas

#### TransformDimClientes

**Origem:** `bronze.contas_base_oficial`
**Destino:** `silver.dim_clientes`
**Tipo:** SCD Type 2

**Transforma√ß√µes aplicadas:**
1. Limpeza de CNPJ/CPF (remove caracteres n√£o num√©ricos)
2. Determina√ß√£o de tipo de pessoa (PF se <= 11 d√≠gitos, PJ se > 11)
3. C√°lculo de tempo como cliente em dias
4. Classifica√ß√£o de porte de empresa (fixo: MEDIO)
5. Classifica√ß√£o de risco (fixo: BAIXO)
6. Formata√ß√£o de data de cria√ß√£o
7. C√°lculo de hash para detec√ß√£o de mudan√ßas

**Valida√ß√µes:**
- CNPJ/CPF n√£o pode ser nulo
- N√£o pode haver CNPJ/CPF duplicados no mesmo batch

#### TransformDimUsuarios

**Origem:** `bronze.usuarios`
**Destino:** `silver.dim_usuarios`
**Tipo:** SCD Type 2

**Transforma√ß√µes aplicadas:**
1. Cria√ß√£o de chave natural a partir do email
2. Padroniza√ß√£o de senioridade
3. Resolu√ß√£o de hierarquia de gestores (auto-relacionamento)
4. Normaliza√ß√£o de canais de vendas
5. C√°lculo de hash para detec√ß√£o de mudan√ßas

**Valida√ß√µes:**
- Email n√£o pode ser nulo
- N√£o pode haver emails duplicados no mesmo batch

#### TransformFactFaturamento

**Origem:** `bronze.faturamento`
**Destino:** `silver.fact_faturamento`
**Tipo:** FULL Load

**Transforma√ß√µes aplicadas:**
1. Convers√£o de data para date
2. Lookup de chaves estrangeiras:
   - `sk_cliente` via JOIN com `dim_clientes` (flag_ativo = TRUE)
   - `sk_usuario` via JOIN com `dim_usuarios` (flag_ativo = TRUE)
   - `sk_data` via JOIN com `dim_tempo`
   - `sk_canal` via JOIN com `dim_canal`
3. C√°lculos de m√©tricas:
   - `valor_bruto` = receita original
   - `valor_liquido` = valor_bruto - valor_desconto
   - `valor_imposto` = valor_bruto * 0.15
   - `valor_comissao` = valor_bruto * 0.05
4. Padroniza√ß√£o de campos (tipo_documento, forma_pagamento)
5. C√°lculo de hash da transa√ß√£o

**Valida√ß√µes:**
- Todas as chaves estrangeiras devem ser resolvidas (n√£o podem ser NULL)
- Valores monet√°rios n√£o podem ser nulos

---

## Qualidade de Dados e Valida√ß√µes

### N√≠veis de Valida√ß√£o

#### Bronze Layer (Permissiva)

A camada Bronze aceita dados problem√°ticos e registra warnings detalhados:

- Valores NULL em campos obrigat√≥rios: ACEITA com WARNING
- Datas inv√°lidas: CONVERTE para NULL com WARNING
- Valores negativos: ACEITA com WARNING
- Duplicatas: ACEITA com WARNING

**Objetivo:** Preservar dados de origem para auditoria e troubleshooting.

#### Silver Layer (Rigorosa)

A camada Silver valida qualidade e REJEITA dados problem√°ticos:

- CNPJ/CPF nulo: REJEITA execu√ß√£o
- CNPJ/CPF duplicado: REJEITA execu√ß√£o
- Chave estrangeira n√£o encontrada: REJEITA execu√ß√£o
- Valores monet√°rios nulos: REJEITA execu√ß√£o

**Objetivo:** Garantir integridade e confiabilidade dos dados anal√≠ticos.

### Testes com Dados Polu√≠dos

O sistema foi testado com dados intencionalmente problem√°ticos para validar comportamento:

**Resultados dos Testes:**

**Bronze - Dados Aceitos:**
- 6 usu√°rios com campos vazios (nome_empresa, Nome, email, senioridade)
- 6 contas com CNPJ nulo, datas inv√°lidas, raz√£o social nula
- 9 faturamentos com datas nulas, valores negativos, moedas inv√°lidas (XXX)

**Silver - Valida√ß√£o Rejeitou:**
- dim_clientes: Execu√ß√£o bloqueada por CNPJ duplicado
- Mensagem: "CNPJs/CPFs duplicados encontrados"

**Conclus√£o:** Sistema detecta e bloqueia dados de baixa qualidade conforme esperado.

### Logs de Qualidade

Todos os ingestores e transformadores registram detalhadamente:

```
INFO: Shape: 10 linhas x 3 colunas
WARNING: Valores nulos detectados:
  - Data: 2 (20.0%)
  - Receita: 2 (20.0%)
  - Moeda: 2 (20.0%)
WARNING: 'data': 3 datas inv√°lidas convertidas para NULL
```

Localiza√ß√£o dos logs: `/app/logs/` (dentro do container) ou `logs/` (no host).

---

## Como Executar o Pipeline

### Pr√©-requisitos

1. Docker e Docker Compose instalados
2. Acesso √† internet para conectar ao PostgreSQL Azure
3. Credenciais do banco de dados configuradas
4. Arquivos CSV dispon√≠veis

### Configura√ß√£o Inicial

#### 1. Clonar o Reposit√≥rio

```bash
git clone https://github.com/brunocredits/credits-dw.git
cd credits-dw
```

#### 2. Configurar Vari√°veis de Ambiente

Copie o arquivo de exemplo e edite com as credenciais reais:

```bash
cp .env.example .env
```

Edite `.env`:
```
DB_HOST=seu_host.postgres.database.azure.com
DB_PORT=5432
DB_NAME=creditsdw
DB_USER=seu_usuario
DB_PASSWORD=sua_senha
```

**IMPORTANTE:** O arquivo `.env` est√° no `.gitignore` e nunca deve ser commitado.

#### 3. Preparar Arquivos CSV

Coloque os arquivos CSV na pasta de entrada:

```bash
# Estrutura esperada:
docker/data/input/onedrive/
‚îú‚îÄ‚îÄ contas_base_oficial.csv
‚îú‚îÄ‚îÄ usuarios.csv
‚îî‚îÄ‚îÄ faturamento.csv
```

**Formato dos CSVs:**

- **Separador:** ponto e v√≠rgula (;)
- **Encoding:** UTF-8
- **Cabe√ßalho:** Primeira linha deve conter nomes das colunas

### Execu√ß√£o Passo a Passo

#### 1. Iniciar o Container

```bash
cd docker
docker compose up -d --build
```

Verificar se o container est√° rodando:
```bash
docker compose ps
```

Deve mostrar `etl-processor` com status `running`.

#### 2. Executar Ingest√£o Bronze

**Op√ß√£o A: Executar todos os ingestores**

```bash
docker compose exec etl-processor python python/run_all_ingestors.py
```

**Op√ß√£o B: Executar ingestores individuais**

```bash
# Ingerir contas
docker compose exec etl-processor python python/ingestors/csv/ingest_contas_base_oficial.py

# Ingerir usu√°rios
docker compose exec etl-processor python python/ingestors/csv/ingest_usuarios.py

# Ingerir faturamento
docker compose exec etl-processor python python/ingestors/csv/ingest_faturamento.py
```

**Sa√≠da esperada:**
```
üöÄ Iniciando ingest√£o: ingest_contas_base_oficial.py
‚úì Arquivo v√°lido | Tamanho: 0.00 MB
‚úì Conex√£o com banco estabelecida
‚úì Arquivo lido com sucesso | 10 linhas
‚úì Todas as colunas obrigat√≥rias est√£o presentes
‚úì Transforma√ß√£o conclu√≠da | 10 registros preparados
‚úì Inser√ß√£o conclu√≠da | 10 linhas inseridas
‚úÖ EXECU√á√ÉO CONCLU√çDA COM SUCESSO
```

#### 3. Executar Transforma√ß√µes Silver

```bash
docker compose exec etl-processor python python/run_silver_transformations.py
```

**Sa√≠da esperada:**
```
=== Executando Transforma√ß√µes Silver ===

‚ñ∂ Executando dim_clientes...
‚úì dim_clientes conclu√≠do

‚ñ∂ Executando dim_usuarios...
‚úì dim_usuarios conclu√≠do

‚ñ∂ Executando fact_faturamento...
‚úì fact_faturamento conclu√≠do

=== Todas transforma√ß√µes conclu√≠das com sucesso ===
```

#### 4. Validar Resultados

Execute queries SQL para verificar os dados:

```sql
-- Contar registros por tabela
SELECT 'bronze.contas_base_oficial' AS tabela, COUNT(*) AS registros FROM bronze.contas_base_oficial
UNION ALL
SELECT 'silver.dim_clientes', COUNT(*) FROM silver.dim_clientes
UNION ALL
SELECT 'silver.fact_faturamento', COUNT(*) FROM silver.fact_faturamento;

-- Verificar integridade referencial
SELECT
    'Fact ‚Üí Dim Clientes' AS relacionamento,
    COUNT(*) AS total,
    COUNT(DISTINCT f.sk_cliente) AS chaves_distintas,
    SUM(CASE WHEN c.sk_cliente IS NULL THEN 1 ELSE 0 END) AS fks_orfas
FROM silver.fact_faturamento f
LEFT JOIN silver.dim_clientes c ON f.sk_cliente = c.sk_cliente;
```

### Execu√ß√£o Agendada (Opcional)

Para executar automaticamente em hor√°rios espec√≠ficos, configure um cron job:

```bash
# Editar crontab
crontab -e

# Executar pipeline completo todo dia √†s 3h da manh√£
0 3 * * * cd /home/usuario/credits-dw/docker && docker compose exec -T etl-processor python python/run_all_ingestors.py && docker compose exec -T etl-processor python python/run_silver_transformations.py
```

---

## Estrutura de Arquivos

```
credits-dw/
‚îÇ
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                    # Imagem Python 3.10 com depend√™ncias
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml            # Orquestra√ß√£o do container
‚îÇ   ‚îî‚îÄ‚îÄ data/
‚îÇ       ‚îú‚îÄ‚îÄ input/onedrive/           # CSVs para processar
‚îÇ       ‚îú‚îÄ‚îÄ processed/                # CSVs j√° processados (arquivados)
‚îÇ       ‚îî‚îÄ‚îÄ templates/                # Exemplos de CSVs
‚îÇ
‚îú‚îÄ‚îÄ python/
‚îÇ   ‚îú‚îÄ‚îÄ ingestors/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ csv/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ base_csv_ingestor.py        # Classe base (Template Method)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ingest_contas_base_oficial.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ingest_usuarios.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ingest_faturamento.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ transformers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_transformer.py             # Classe base Silver
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ silver/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ transform_dim_clientes.py   # SCD Type 2
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ transform_dim_usuarios.py   # SCD Type 2
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ transform_fact_faturamento.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py                 # Configura√ß√µes centralizadas (dataclasses)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db_connection.py          # Gerenciamento de conex√µes (context managers)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.py                 # Configura√ß√£o Loguru
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ audit.py                  # Fun√ß√µes de auditoria
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ run_all_ingestors.py          # Script para executar todos Bronze
‚îÇ   ‚îî‚îÄ‚îÄ run_silver_transformations.py # Script para executar todos Silver
‚îÇ
‚îú‚îÄ‚îÄ logs/                              # Logs de execu√ß√£o (criado automaticamente)
‚îÇ
‚îú‚îÄ‚îÄ .env                               # Credenciais (N√ÉO versionar, est√° no .gitignore)
‚îú‚îÄ‚îÄ .env.example                       # Template de .env
‚îú‚îÄ‚îÄ requirements.txt                   # Depend√™ncias Python
‚îú‚îÄ‚îÄ README.md                          # Este arquivo
‚îî‚îÄ‚îÄ CLAUDE.md                          # Documenta√ß√£o t√©cnica para Claude Code
```

### Arquivos de Configura√ß√£o

#### requirements.txt

Lista de pacotes Python instalados no container:

```
pandas==2.1.4          # Manipula√ß√£o de DataFrames
psycopg2-binary==2.9.9 # Driver PostgreSQL
loguru==0.7.2          # Sistema de logs avan√ßado
tenacity==8.2.3        # Retry logic com exponential backoff
python-dotenv==1.0.0   # Carregamento de vari√°veis de ambiente
```

#### docker-compose.yml

Configura√ß√£o do servi√ßo ETL:

```yaml
services:
  etl-processor:
    build: .
    container_name: credits-etl
    volumes:
      - ../python:/app/python
      - ../logs:/app/logs
      - ./data:/app/data
    environment:
      - DB_HOST=${DB_HOST}
      - DB_PORT=${DB_PORT}
      - DB_NAME=${DB_NAME}
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
      - TZ=America/Sao_Paulo
```

---

## Troubleshooting

### Container n√£o inicia

**Sintoma:** `docker compose up -d` falha ou container para imediatamente.

**Diagn√≥stico:**
```bash
docker compose logs etl-processor
```

**Solu√ß√µes:**
1. Verificar se o arquivo `.env` existe e est√° configurado
2. Rebuild for√ßado: `docker compose down && docker compose up -d --build`
3. Verificar portas em uso: `docker compose ps`

### Erro de conex√£o ao banco de dados

**Sintoma:** `connection refused` ou `timeout`

**Causas comuns:**
1. Firewall do Azure bloqueando seu IP
2. Credenciais incorretas no `.env`
3. Banco de dados fora do ar

**Diagn√≥stico:**
```bash
# Testar conex√£o de dentro do container
docker compose exec etl-processor python -c "
from utils.db_connection import get_connection
with get_connection() as conn:
    print('Conex√£o OK')
"
```

**Solu√ß√µes:**
1. Adicionar seu IP no firewall do Azure PostgreSQL
2. Verificar credenciais: usu√°rio, senha, nome do banco
3. Testar com `psql` ou ferramenta GUI (DBeaver, pgAdmin)

### Ingestor falha com "Colunas obrigat√≥rias faltando"

**Sintoma:**
```
ValueError: Colunas obrigat√≥rias faltando: {'Data de cria√ß√£o', 'CNPJ/CPF PK'}
```

**Causa:** Nome das colunas no CSV n√£o corresponde ao mapeamento no ingestor.

**Solu√ß√£o:**
1. Abrir o CSV e verificar os nomes exatos das colunas
2. Comparar com `get_column_mapping()` no arquivo do ingestor
3. Ajustar o CSV ou o mapeamento conforme necess√°rio

**Exemplo:**
```python
# Ingestor espera:
'CNPJ/CPF PK': 'cnpj_cpf'

# Mas CSV tem:
'CNPJ / CPF'  # <-- Espa√ßos extras

# Solu√ß√£o: Ajustar header do CSV ou mapeamento
```

### Transforma√ß√£o Silver falha com "Valida√ß√£o falhou"

**Sintomas poss√≠veis:**
- "CNPJs/CPFs duplicados encontrados"
- "sk_cliente nulo - dim_clientes vazia?"
- "Datas n√£o encontradas na dim_tempo"

**Diagn√≥stico:**
```sql
-- Verificar se Bronze tem dados
SELECT COUNT(*) FROM bronze.contas_base_oficial;

-- Verificar duplicatas
SELECT cnpj_cpf, COUNT(*)
FROM bronze.contas_base_oficial
GROUP BY cnpj_cpf
HAVING COUNT(*) > 1;

-- Verificar dim_tempo
SELECT MIN(data_completa), MAX(data_completa), COUNT(*)
FROM silver.dim_tempo;
```

**Solu√ß√µes:**
1. **Duplicatas:** Limpar dados de origem, remover duplicatas antes de reprocessar
2. **Dimens√µes vazias:** Executar ingest√£o Bronze primeiro
3. **Datas fora do range:** Estender dim_tempo ou ajustar datas nos CSVs

### Logs n√£o aparecem

**Sintoma:** Pasta `logs/` vazia ou arquivos n√£o criados.

**Causa:** Permiss√µes de escrita ou volume n√£o montado.

**Solu√ß√£o:**
```bash
# Verificar se pasta existe
ls -la logs/

# Criar manualmente se necess√°rio
mkdir -p logs
chmod 777 logs

# Verificar dentro do container
docker compose exec etl-processor ls -la /app/logs
```

### Arquivo CSV n√£o √© encontrado

**Sintoma:** `Arquivo n√£o encontrado: /app/data/input/onedrive/usuarios.csv`

**Causa:** Arquivo n√£o est√° no local esperado ou nome incorreto.

**Solu√ß√£o:**
```bash
# Verificar conte√∫do da pasta
docker compose exec etl-processor ls -la /app/data/input/onedrive/

# Copiar arquivo para local correto
cp meu_arquivo.csv docker/data/input/onedrive/usuarios.csv

# Verificar nome do arquivo no ingestor
# Deve corresponder exatamente ao par√¢metro arquivo_nome no __init__
```

### Performance lenta

**Sintoma:** Ingest√£o ou transforma√ß√£o demora muito tempo.

**Causas comuns:**
1. Arquivos CSV muito grandes
2. Rede lenta para Azure
3. Queries sem √≠ndices

**Solu√ß√µes:**
1. Processar em batches menores
2. Usar conex√£o de rede mais r√°pida
3. Criar √≠ndices no banco:
```sql
CREATE INDEX idx_clientes_nk ON silver.dim_clientes(nk_cnpj_cpf);
CREATE INDEX idx_usuarios_nk ON silver.dim_usuarios(nk_usuario);
CREATE INDEX idx_tempo_data ON silver.dim_tempo(data_completa);
```

---

## Gloss√°rio T√©cnico

### Conceitos de Arquitetura

**Medallion Architecture**
Padr√£o de arquitetura de Data Lake/Warehouse que organiza dados em camadas incrementais: Bronze (raw), Silver (curated), Gold (aggregated).

**Bronze Layer (Camada Bronze)**
Primeira camada que armazena dados brutos com transforma√ß√µes m√≠nimas. Objetivo √© preservar dados de origem para auditoria.

**Silver Layer (Camada Silver)**
Segunda camada que armazena dados limpos, transformados e modelados. Otimizada para an√°lises e consultas.

**Gold Layer (Camada Gold)**
Terceira camada (n√£o implementada neste projeto) que cont√©m agrega√ß√µes, KPIs e m√©tricas de neg√≥cio pr√©-calculadas.

### Conceitos de Modelagem

**Star Schema (Esquema Estrela)**
Modelo dimensional onde uma tabela fato central √© conectada a m√∫ltiplas tabelas dimens√£o, formando uma estrela.

**Fact Table (Tabela Fato)**
Tabela central do Star Schema que cont√©m m√©tricas num√©ricas (fatos) e chaves estrangeiras para dimens√µes.

**Dimension Table (Tabela Dimens√£o)**
Tabela que cont√©m atributos descritivos e contextuais para an√°lise (quem, quando, onde, como).

**Surrogate Key (Chave Substituta)**
Chave prim√°ria artificial (geralmente autoincrement integer) que substitui chaves naturais. Prefixo: `sk_`.

**Natural Key (Chave Natural)**
Chave que existe naturalmente nos dados de neg√≥cio (CNPJ, CPF, email). Prefixo: `nk_`.

**Foreign Key (Chave Estrangeira)**
Coluna que referencia a chave prim√°ria de outra tabela. Prefixo: `sk_` (pois referenciam surrogate keys).

### Conceitos de ETL

**ETL (Extract, Transform, Load)**
Processo de extrair dados de origens, transform√°-los conforme regras de neg√≥cio e carreg√°-los em destino.

**Ingestor**
Script respons√°vel por ler dados de origem (CSV) e carreg√°-los na camada Bronze.

**Transformer (Transformador)**
Script respons√°vel por transformar dados da Bronze para Silver aplicando regras de neg√≥cio.

**Template Method**
Padr√£o de design que define o esqueleto de um algoritmo em uma classe base, permitindo que subclasses implementem etapas espec√≠ficas.

**Context Manager**
Padr√£o Python que garante setup e cleanup de recursos (conex√µes, arquivos) usando `with` statement.

**TRUNCATE/RELOAD**
Estrat√©gia de carga que remove todos os registros da tabela e insere novamente. Usado em Bronze.

**FULL Load**
Estrat√©gia de carga que substitui completamente os dados da tabela de destino.

**Incremental Load**
Estrat√©gia de carga que adiciona apenas registros novos ou modificados.

### Conceitos de SCD

**SCD (Slowly Changing Dimension)**
T√©cnica para rastrear mudan√ßas em dimens√µes ao longo do tempo.

**SCD Type 1**
Sobrescreve dados antigos com novos. N√£o mant√©m hist√≥rico.

**SCD Type 2**
Mant√©m hist√≥rico criando novos registros para cada mudan√ßa. Usa campos de controle:
- `data_inicio` / `data_fim`: Per√≠odo de validade
- `flag_ativo`: Indica vers√£o atual (TRUE) ou hist√≥rica (FALSE)
- `versao`: N√∫mero sequencial da vers√£o
- `hash_registro`: MD5 dos dados para detectar mudan√ßas

**SCD Type 3**
Mant√©m apenas vers√£o anterior em colunas separadas (ex: `status_atual`, `status_anterior`).

### Conceitos de Qualidade

**Data Quality (Qualidade de Dados)**
Grau em que os dados atendem requisitos de completude, consist√™ncia, precis√£o e integridade.

**Dirty Data (Dados Polu√≠dos)**
Dados com problemas de qualidade: valores NULL, duplicatas, formatos inv√°lidos, valores fora do range esperado.

**Referential Integrity (Integridade Referencial)**
Garantia de que chaves estrangeiras referenciam registros existentes nas tabelas dimens√£o.

**Hash MD5**
Algoritmo que gera uma string √∫nica de 32 caracteres a partir de dados. Usado para detectar mudan√ßas em SCD Type 2.

### Conceitos de Banco de Dados

**Schema (Esquema)**
Namespace que agrupa tabelas relacionadas. Exemplos: `bronze`, `silver`, `credits`.

**TRUNCATE**
Comando SQL que remove todos os registros de uma tabela (mais r√°pido que DELETE).

**CASCADE**
Op√ß√£o que propaga opera√ß√µes para tabelas relacionadas (ex: TRUNCATE CASCADE remove dados de tabelas filhas).

**Index (√çndice)**
Estrutura de dados que acelera consultas criando uma "tabela de busca" para uma ou mais colunas.

**Constraint (Restri√ß√£o)**
Regra aplicada a colunas (PRIMARY KEY, FOREIGN KEY, NOT NULL, UNIQUE).

### Conceitos de Python

**DataFrame**
Estrutura de dados tabular do pandas (biblioteca Python) usada para manipular dados.

**Dataclass**
Classe Python que automatiza cria√ß√£o de `__init__`, `__repr__` e outros m√©todos. Usado em `utils/config.py`.

**Type Hints**
Anota√ß√µes de tipo em Python (ex: `def funcao(x: int) -> str`) que melhoram legibilidade e permitem valida√ß√£o.

**Context Manager**
Objeto que implementa `__enter__` e `__exit__` para gerenciar recursos. Usado com `with`.

### Ferramentas e Tecnologias

**Docker**
Plataforma de conteineriza√ß√£o que empacota aplica√ß√µes e depend√™ncias em ambientes isolados.

**Docker Compose**
Ferramenta para definir e executar aplica√ß√µes Docker multi-container usando arquivo YAML.

**PostgreSQL**
Sistema de gerenciamento de banco de dados relacional open-source.

**Loguru**
Biblioteca Python de logging com recursos avan√ßados (cores, rota√ß√£o, compress√£o).

**Tenacity**
Biblioteca Python para implementar retry logic com exponential backoff.

**pandas**
Biblioteca Python para an√°lise e manipula√ß√£o de dados tabulares.

**psycopg2**
Driver PostgreSQL para Python.

---

## Perguntas Frequentes (FAQ)

**Q: Posso executar os scripts fora do Docker?**
R: Sim, mas voc√™ precisa instalar Python 3.10+, criar um virtualenv e instalar depend√™ncias via `pip install -r requirements.txt`. Tamb√©m precisa ajustar os paths de arquivos (remover `/app/`).

**Q: Como adicionar uma nova coluna a uma tabela existente?**
R: 1) Adicionar coluna no banco via `ALTER TABLE`, 2) Atualizar `get_column_mapping()` no ingestor, 3) Atualizar `get_bronze_columns()`, 4) Atualizar transformador Silver para processar nova coluna.

**Q: √â poss√≠vel reprocessar apenas um dia de dados?**
R: Atualmente n√£o, pois Bronze usa TRUNCATE/RELOAD. Para processar incrementalmente, voc√™ precisaria mudar a estrat√©gia para INSERT com valida√ß√£o de duplicatas.

**Q: Como conectar o Power BI ao Data Warehouse?**
R: Use o conector PostgreSQL do Power BI. Configure: Host (Azure), Port (5432), Database (creditsdw), Usu√°rio, Senha. Aponte para o schema `silver`.

**Q: Os logs s√£o persistentes?**
R: Sim, logs ficam na pasta `logs/` que est√° montada como volume Docker. S√£o rotacionados a cada 100MB e mantidos por 30 dias.

**Q: Posso executar transforma√ß√µes em paralelo?**
R: N√£o √© recomendado pois fact_faturamento depende das dimens√µes. Execute na ordem: dim_clientes, dim_usuarios, fact_faturamento.

**Q: Como fazer backup dos dados?**
R: Use `pg_dump` para backup do banco PostgreSQL. Exemplo:
```bash
pg_dump -h creditsdw.postgres.database.azure.com -U creditsdw -d creditsdw -n bronze -n silver > backup.sql
```

---

## Contatos e Suporte

Para d√∫vidas, problemas ou sugest√µes relacionadas ao Data Warehouse:

**Equipe de Engenharia de Dados - Credits Brasil**

- Reposit√≥rio GitHub: [github.com/brunocredits/credits-dw](https://github.com/brunocredits/credits-dw)
- Abra uma Issue para reportar bugs ou solicitar features

---

**√öltima atualiza√ß√£o:** Novembro 2025
**Vers√£o da documenta√ß√£o:** 3.0
**Mantenedor:** Equipe de Engenharia de Dados
